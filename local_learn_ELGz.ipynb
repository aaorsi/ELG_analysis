{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a version of learn_ELGz.ipynb intended to work with processed instead of raw catalogues.\n",
    "\n",
    "## I use it for working locally\n",
    "\n",
    "$x = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j-plus [INFO]: Fetching  J-PLUS filters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting redshift ranges of ELGs\n",
      "[0.7482142815072982, 0.7964311035074512] [-0.007223125525262806, 0.020158269506669296] [0.3403815320258589, 0.3773500766863342] [0.30129710948226485, 0.3371876817999342]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "import numpy as np\n",
    "import jplus\n",
    "import elgtools as elg\n",
    "import MockJPLUS as mtools\n",
    "import matplotlib.pyplot as plt\n",
    "import Read_Mocks as read\n",
    "\n",
    "DataDir = './data'\n",
    "\n",
    "\n",
    "print 'setting redshift ranges of ELGs'\n",
    "f_j0660 = jplus.datasets.fetch_jplus_filter('J0660') # Filter transmission curve\n",
    "\n",
    "w_oii = 3727.0 # OII rest-frame\n",
    "z_oii = elg.zline(w_oii, f_j0660.wave, f_j0660.throughput)\n",
    "\n",
    "w_ha = 6563.0\n",
    "z_ha = elg.zline(w_ha, f_j0660.wave, f_j0660.throughput)\n",
    "\n",
    "w_hb = 4861.0\n",
    "z_hb = elg.zline(w_hb, f_j0660.wave, f_j0660.throughput)\n",
    "        \n",
    "w_oiii = 5007.0\n",
    "z_oiii = elg.zline(w_oiii, f_j0660.wave, f_j0660.throughput)\n",
    "\n",
    "print z_oii, z_ha, z_hb, z_oiii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.core import multiarray \n",
    "import pickle\n",
    "snrfile = '%s/%s' % (DataDir,'snr.data')\n",
    "snrdata = pickle.load(open(snrfile,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print pickle.format_version\n",
    "print np.version.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute sigma curves for each tile\n",
    "import pickle\n",
    "\n",
    "Load_GalJPlus = False\n",
    "\n",
    "if Load_GalJPlus:\n",
    "    raw_xmatch_file = '%s/%s'%(DataDir, 'jplus_sdssphot.data')\n",
    "    gal_jplus = pickle.load(open(raw_xmatch_file))\n",
    "\n",
    "    print gal_jplus.keys()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "ComputeSigmas = False\n",
    "import matplotlib\n",
    "\n",
    "suff ='SDSS'\n",
    "\n",
    "import pickle\n",
    "import matplotlib.gridspec as gsc\n",
    "if ComputeSigmas:\n",
    "    \n",
    "    reload(elg)\n",
    "    alltiles = np.random.permutation(np.unique(gal_jplus['tile_id']))\n",
    "    ntiles = len(alltiles)\n",
    "    print 'Number of tiles:%d'%ntiles\n",
    "    sigma_tiles = []\n",
    "\n",
    "    npanels = 5\n",
    "\n",
    "    gs = gsc.GridSpec(npanels,1)\n",
    "    gs.update(wspace=0.0, hspace=0.0, top=2)\n",
    "    gal_jplus['ids'] = np.arange(0,len(gal_jplus['tile_id']))\n",
    "    gal_jplus['elgcand'] = np.zeros(len(gal_jplus['tile_id']))\n",
    "    gal_jplus['dm_j0660'] = np.zeros(len(gal_jplus['tile_id']))\n",
    "    gal_jplus['dm_j0660_err'] = np.zeros(len(gal_jplus['tile_id']))\n",
    "    nc = 0\n",
    "    for i in range(ntiles):\n",
    "        print i,\n",
    "    #for i in range(20):\n",
    "        if i < npanels:\n",
    "            ax = plt.subplot(gs[i])\n",
    "        \n",
    "        idcand,dm, dm_err, sfunc = elg.continuum_curve(gal_jplus, alltiles[i], BroadLineName = 'r'+suff,\n",
    "                 BroadNoLineName='g'+suff, Plot=ax if i < npanels else False,\n",
    "                 sigma_threshold = 2.0)\n",
    "        sigma_tiles.append(sfunc)\n",
    "        \n",
    "        gal_jplus['elgcand'][idcand] = 1\n",
    "        gal_jplus['dm_j0660'][idcand] = dm\n",
    "        gal_jplus['dm_j0660_err'][idcand] = dm_err\n",
    "        if i == npanels:\n",
    "            ax.set_xlabel(r'$r%s$'%suff, fontsize=25)\n",
    "        if i == 2:\n",
    "            ax.set_ylabel(r'$C^{r,g}-J0660$', fontsize=25)\n",
    "        #plt.title('TileID: %d'%alltiles[i],fontsize=15)\n",
    "        nc += len(idcand)\n",
    "        \n",
    "\n",
    "        #Save file with sigma curves\n",
    "\n",
    "        sigma_dict = {'sfunc': sigma_tiles, 'tiles':alltiles}\n",
    "        sigmafile = 'sigma_curves.data'\n",
    "\n",
    "        with open(sigmafile,'wb') as outfile:\n",
    "            pickle.dump(sigma_dict,outfile,protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "\n",
    "\n",
    "\n",
    "    elg_cand = jplus.tools.select_object(gal_jplus,gal_jplus['elgcand'] == 1)\n",
    "    print 'saving candidate list...'\n",
    "    \n",
    "    with open('%s/elg_cand.data'%DataDir, 'wb') as outfile:\n",
    "        pickle.dump(elg_cand, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    elg_cand = pickle.load(open('%s/elg_cand.data'%DataDir))\n",
    "\n",
    "print 'Number of ELG candidates: %d'%len(elg_cand['tile_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose which base catalogue to use when constructing the dataset\n",
    "dc = elg_cand\n",
    "suff = 'SDSS'\n",
    "\n",
    "dm, fline, fcont = mtools.gen_3fm(dc['J0660'][:,0],  \n",
    "                    dc['r'+suff][:,0], dc['g'+suff][:,0], \n",
    "                    Broad_NoLineName='g'+suff)\n",
    "\n",
    "mask = ((dc['dm_j0660'] > 0.4) & # a minimum EW limit -- not sure what this corresponds to\n",
    "        (dc['r'+suff][:,0] > 20) &  # remove most of H-alpha contaminants\n",
    "        (dc['mask_flags_J0660'] == 0) & # no artifacts is J0660 photometry\n",
    "        (dc['mask_flags_rJAVA'] == 0) &\n",
    "        (dc['single_detect_J0660'] != 0) & # Objects are detected in both rJAVA and J0660\n",
    "        (1./dc['J0660'][:,1] > 5) &    # SNR of J0660 is above 5\n",
    "         (1./dc['r'+suff][:,1] > 5) &  # SNR of SDSS r-band is above 5\n",
    "        (1./dc['rJAVA'][:,1] > 5)     # also SNR of rJAVA is above 5\n",
    "         )\n",
    "\n",
    "dcat = jplus.tools.select_object(dc, mask)\n",
    "dcat['F_j0660'] = fline[mask]\n",
    "dcat['F_cont_j0660'] = fcont[mask]\n",
    "print 'Total number of J-PLUS objects for cross-matches: %d'%len(dcat['tile_id'])\n",
    "\n",
    "Remove_dc = True\n",
    "if Remove_dc:\n",
    "    del dc\n",
    "    #del elg_cand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "plt.plot([1,1],[1,1])\n",
    "#plt.hist(np.log10(np.asarray(dcat['F_j0660'])))\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "#plt.title('raw flux counts distribution')\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dcat['dm_j0660'], bins=20, color='green', alpha=0.5,range=[-2,2])\n",
    "\n",
    "\n",
    "plt.xlabel(r'$C^{r,i} - J0660$',fontsize=20)\n",
    "plt.xlim([-2,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "if suff == 'SDSS': # check cross-match by comparing rJAVA to rSDSS\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import LogNorm\n",
    "    plt.rc('font', family='STIXGeneral')\n",
    "\n",
    "    gs = gsc.GridSpec(2,1)\n",
    "    gs.update(hspace=0.2)\n",
    "    \n",
    "    xr = [19,21.9]\n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    nm = 20\n",
    "    marr   = np.linspace(xr[0],xr[1],nm)\n",
    "    mb = marr[1] - marr[0]\n",
    "    ddm    = np.zeros([nm,3])\n",
    "    dem    = np.zeros([nm,3])\n",
    "    deltam = elg_cand['rJAVA'][:,0] - elg_cand['rSDSS'][:,0]\n",
    "    ratio  = elg_cand['rJAVA'][:,1]/elg_cand['rSDSS'][:,1]\n",
    "    mg = elg_cand['rSDSS'][:,0]\n",
    "    counts, ybins, xbins, image = ax.hist2d(mg,deltam,\n",
    "                                    bins=100, cmap=plt.cm.Greys, normed=LogNorm(), cmin=0.01, label='SDSS contaminants', \n",
    "                                    alpha=0.9,range=(xr,[-.75,.749]))\n",
    "    \n",
    "    for i in range(nm):\n",
    "        sel = (mg > marr[i]-mb/2.) & (mg <= marr[i]+mb/2.)\n",
    "        dd_i  = deltam[sel]\n",
    "        err_i = ratio[sel]\n",
    "        \n",
    "        if len(dd_i) > 0:\n",
    "            ddm[i,0] = np.percentile(dd_i,50)\n",
    "            ddm[i,1] = np.percentile(dd_i,20)\n",
    "            ddm[i,2] = np.percentile(dd_i,80)\n",
    "        if len(err_i) > 0:\n",
    "            dem[i,0] = np.percentile(err_i,50)\n",
    "            dem[i,1] = np.percentile(err_i,20)\n",
    "            dem[i,2] = np.percentile(err_i,80)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    ax.plot(xr,[0,0],'--k,')\n",
    "    \n",
    "    ax.fill_between(marr, ddm[:,1],ddm[:,2],facecolor='RoyalBlue',alpha=0.2)\n",
    "    #ax.plot(marr,ddm[:,0],'.-',color='RoyalBlue',linewidth=3)\n",
    "    ax.set_ylim([-.75,.749])\n",
    "    ax.set_xlim(xr)\n",
    "    \n",
    "    ax.set_ylabel(r'$rJAVA - rSDSS$',fontsize=15)\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "    ax = plt.subplot(gs[1])\n",
    "    \n",
    "    yl = [.1,2.9]\n",
    "    \n",
    "    counts, ybins, xbins, image = ax.hist2d(mg,ratio,\n",
    "                                    bins=100, cmap=plt.cm.Greys, normed=LogNorm(), cmin=0.01, label='SDSS contaminants', \n",
    "                                    alpha=0.9,range=(xr,yl))\n",
    "\n",
    "    ax.plot(xr,[1,1],'--k,')\n",
    "    ax.set_ylim(yl)\n",
    "    ax.set_xlim(xr)\n",
    "    ax.fill_between(marr, dem[:,1],dem[:,2],facecolor='RoyalBlue',alpha=0.2)\n",
    "   # ax.plot(marr,dem[:,0],'.-',color='RoyalBlue',linewidth=3)\n",
    "\n",
    "    ax.set_ylabel(r'$\\sigma_{rJAVA}/\\sigma_{rSDSS}$',fontsize=15)\n",
    "    ax.set_xlabel(r'$r{\\rm SDSS}$',fontsize=15)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    print elg_cand.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Loading 3D-HST and DEEP2'\n",
    "# Loading 3D-HST and DEEP2 DR4 data\n",
    "\n",
    "hstdata = hst.select_3DHST_z(0,5)#,LineName='Ha_flux',LineMin=0.001)\n",
    "nhst = len(hstdata['ra'])\n",
    "hst_coords = np.zeros([nhst,2])\n",
    "\n",
    "for i in range(nhst):\n",
    "    hst_coords[i,:] = [hstdata['ra'][i],hstdata['dec'][i]]\n",
    "hstdata['coords'] = hst_coords\n",
    "\n",
    "deep2_data = '/home/CEFCA/aaorsi/work/elg_jplus/spec/deep2/dr4/zcat.deep2.dr4.fits'\n",
    "deep2 = fits.open(deep2_data)[1].data\n",
    "\n",
    "\n",
    "# XMatch with 3DHST\n",
    "print 'xmatch with 3DHST'\n",
    "d,ind = jplus.tools.crossmatch_angular(dcat['coords'],hstdata['coords'],max_distance=3e-3)\n",
    "m = ((d != np.inf))\n",
    "\n",
    "jhst0 = jplus.tools.select_object(dcat, m)\n",
    "jhst0['z_spec'] = hstdata['z'][ind[m]]\n",
    "\n",
    "elg_hst =  (\n",
    "            (((jhst0['z_spec'] > z_ha[0])   & (jhst0['z_spec'] < z_ha[1]))   |\n",
    "           ((jhst0['z_spec'] > z_hb[0])   & (jhst0['z_spec'] < z_hb[1]))   |\n",
    "           ((jhst0['z_spec'] > z_oiii[0]) & (jhst0['z_spec'] < z_oiii[1])) |\n",
    "           ((jhst0['z_spec'] > z_oii[0])  & (jhst0['z_spec'] < z_oii[1]))))\n",
    "\n",
    "jhst = jplus.tools.select_object(jhst0, elg_hst)\n",
    "nhst = len(jhst['z_spec'])\n",
    "jhst['idd'] = np.arange(nhst)\n",
    "\n",
    "\n",
    "# xmatch with DEEP2\n",
    "print 'xmatch with DEEP2'\n",
    "d2mask = ((deep2['ZQUALITY'] > 2) &  # Select only Deep2 objects with good quality Redshifts\n",
    "        (deep2['Z'] <= z_ha[1]) |\n",
    "           ((deep2['Z'] >= z_hb[0])   &   (deep2['Z'] <= z_hb[1]))   |\n",
    "           ((deep2['Z'] >= z_oiii[0]) &   (deep2['Z'] <= z_oiii[1])) |\n",
    "           ((deep2['Z'] >= z_oii[0])  &   (deep2['Z'] <= z_oii[1])))\n",
    "          \n",
    "          \n",
    "ndeep2 = len(deep2['RA'][d2mask])\n",
    "dcoords = np.asarray([[deep2['RA'][d2mask][i], deep2['DEC'][d2mask][i]] for i in range(ndeep2)])\n",
    "d,ind = jplus.tools.crossmatch_angular(dcat['coords'],dcoords,max_distance=3e-3)\n",
    "m = ((d != np.inf))\n",
    "\n",
    "jdeep2 = jplus.tools.select_object(dcat, m )\n",
    "jdeep2['z_spec'] = deep2['Z'][d2mask][ind[m]]\n",
    "ndeep2 = len(jdeep2['z_spec'])  \n",
    "#jdeep2['idd'] = np.arange(ndeep2)\n",
    "print jdeep2['z_spec']\n",
    "print jhst['z_spec']\n",
    "print 'Done loading data!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Reading and performing cross-match with ALHAMBRA objects'\n",
    "import deepdish as dd\n",
    "AlhambraFile = '/home/CEFCA/aaorsi/work/alhambra/Alhambra.hdf5'\n",
    "\n",
    "alh = dd.io.load(AlhambraFile)\n",
    "print alh.keys()\n",
    "\n",
    "\n",
    "alh_d,alh_ind = jplus.tools.crossmatch_angular(dcat['coords'],alh['coords'],max_distance=3e-4)\n",
    "alh_m = ((alh_d != np.inf))\n",
    "\n",
    "j_alh = jplus.tools.select_object(dcat, alh_m)\n",
    "print 'Cross match between Alhambra and J-PLUS results in %ld objects' % len(j_alh['tile_id'])\n",
    "j_alh['z_spec'] = alh['zphoto'][alh_ind[alh_m]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 5, 5\n",
    "\n",
    "mask_elgs =  (\n",
    "            (((j_alh['z_spec'] > z_ha[0])   & (j_alh['z_spec'] < z_ha[1]))   |\n",
    "           ((j_alh['z_spec'] > z_hb[0])   & (j_alh['z_spec'] < z_hb[1]))   |\n",
    "           ((j_alh['z_spec'] > z_oiii[0]) & (j_alh['z_spec'] < z_oiii[1])) |\n",
    "           ((j_alh['z_spec'] > z_oii[0])  & (j_alh['z_spec'] < z_oii[1]))))\n",
    "\n",
    "\n",
    "plt.hist(j_alh['z_spec'][mask_elgs], bins=50)\n",
    "nalh = len(j_alh['z_spec'][mask_elgs])\n",
    "\n",
    "j_alhambra = jplus.tools.select_object(j_alh, mask_elgs)\n",
    "\n",
    "print nalh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.gridspec as gsc\n",
    "\n",
    "ngals_tot = ndeep2 + nhst + nsdss + nalh + ncontaminants\n",
    "print 'Total number of objects in data set: %d'%ngals_tot\n",
    "\n",
    "dataspec = {}\n",
    "\n",
    "for key in jdeep2:\n",
    "    if key == 'date' or key == 'SQL_query' or key == 'filename':\n",
    "        continue\n",
    "    counter = 0\n",
    "    value_hst   = jhst[key]\n",
    "    value_deep2 = jdeep2[key]\n",
    "    value_sdss  = j_sdss[key]\n",
    "    value_alh   = j_alhambra[key]\n",
    "    value_int   = j_sdss_interlopers[key]\n",
    "    value_star  = s_spec[key]\n",
    "    value_qso   = q_spec[key]\n",
    "    shape = value_deep2.shape\n",
    "    ndim = value_deep2.ndim\n",
    "    dataspec[key] = np.zeros(ngals_tot) if ndim == 1 else np.zeros([ngals_tot, shape[1]])\n",
    "    counter += ndeep2\n",
    "    dataspec[key][0:counter] = value_deep2\n",
    "    dataspec[key][counter:counter+nhst] = value_hst   \n",
    "    counter += nhst\n",
    "    dataspec[key][counter:counter+nsdss] = value_sdss   \n",
    "    counter += nsdss\n",
    "    dataspec[key][counter:counter+nalh] = value_alh   \n",
    "    counter += nalh\n",
    "    dataspec[key][counter:counter+nstars] = value_star   \n",
    "    counter += nstars\n",
    "    dataspec[key][counter:counter+nqso] = value_qso   \n",
    "    counter += nqso\n",
    "    dataspec[key][counter:counter+nsdss_inter] = value_int   \n",
    "    counter += nsdss_inter\n",
    "    \n",
    "    \n",
    "\n",
    "dataspec['type'] = []\n",
    "for i in range(ndeep2):\n",
    "    dataspec['type'].append('DEEP2')\n",
    "for i in range(nhst):\n",
    "    dataspec['type'].append('3D-HST')\n",
    "for i in range(nsdss):\n",
    "    dataspec['type'].append('SDSS')\n",
    "for i in range(nalh):\n",
    "    dataspec['type'].append('Alhambra')\n",
    "for i in range(ncontaminants):\n",
    "    dataspec['type'].append('SDSS contaminants')\n",
    "    \n",
    "dataspec['index'] = np.arange(counter)    \n",
    "print counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Lightcone galaxies\n",
    "LoadLightCone = False\n",
    "if LoadLightCone:\n",
    "    nCone = 512\n",
    "    lcone = []\n",
    "    reload(read)\n",
    "    #Mockpath = '/home/CEFCA/aaorsi/work/JPLUS_Mock/new/Lines/'\n",
    "    Mockpath = '/home/CEFCA/aaorsi/work/JPLUS_Mock/new_0918/'\n",
    "    namelc   = 'LightCone_SA_0_'\n",
    "\n",
    "    print 'reading Lightcone...'\n",
    "    for i in range(nCone): #nCone):\n",
    "        nameIn = '%s%s%d' % (Mockpath, namelc, i)\n",
    "        ilc = read.readmock_chunk_PythonCut(nameIn, zspace = True)\n",
    "        lcone.append(ilc[0])\n",
    "\n",
    "    dcone = np.concatenate(lcone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LoadLightCone:\n",
    "    dcone[0]['ObsMagDust']\n",
    "    print dcone.dtype\n",
    "    names_filters = ['uJAVA' , 'J0378', 'J0395', 'J0410', 'J0430', 'gSDSS',\n",
    "                             'J0515', 'rSDSS', 'J0660', 'iSDSS', 'J0861', 'zSDSS']\n",
    "\n",
    "    nf = len(names_filters)\n",
    "    mockgals = {}\n",
    "\n",
    "    for _i in range(nf):\n",
    "        fname = names_filters[_i]\n",
    "        mockgals[fname] = dcone['ObsMagDust'][_i,:]\n",
    "\n",
    "    mockgals['redshift'] = dcone['redshift']\n",
    "\n",
    "    print mockgals.keys()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataspec\n",
    "\n",
    "dfile = '%s/data4training.data'%DataDir\n",
    "\n",
    "dd = pickle.load(open(dfile,'rb'))\n",
    "\n",
    "print dd.keys()\n",
    "dataspec = dd['dataspec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot (some) dataset objects\n",
    "\n",
    "Plot_TrainingSet = True\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if Plot_TrainingSet:\n",
    "\n",
    "    filternames = [ 'J0378','J0395', 'J0410','J0430','J0515', 'J0660',\n",
    "                   'J0861','uJAVA', 'gJAVA', 'rJAVA', 'iJAVA','zJAVA']\n",
    "                #  'uSDSS', 'gSDSS', 'rSDSS', 'iSDSS', 'zSDSS']\n",
    "\n",
    "    sdss_fnames = ['uSDSS', 'gSDSS', 'rSDSS', 'iSDSS', 'zSDSS']\n",
    "    sdss_mw     = [3543, 4770, 6231, 7625, 9134]\n",
    "\n",
    "  \n",
    "    grid = [4,4]\n",
    "    gs = gsc.GridSpec(grid[0],grid[1])\n",
    "    gs.update(wspace=0.035, hspace=0.035)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = 15, 15\n",
    "\n",
    "    \n",
    "    nxm = grid[0]*grid[1]\n",
    "    print 'Number of xmatched ELGs: %ld'%nxm\n",
    "    \n",
    "    ix = 0\n",
    "    iy = 0\n",
    "    idg = 0\n",
    "    \n",
    "    types = np.unique(dataspec['class'])\n",
    "    sarr= np.unique(dataspec['type'])\n",
    "    \n",
    "     \n",
    "    \n",
    "    ssarr = [r'$H\\alpha$',r'$(H\\beta, [OIII])$',r'$[OII]$','Other']\n",
    "    #types[-1] = 'SDSS'\n",
    "    sortz = []\n",
    "    ichoice = []\n",
    "    for ii in types:\n",
    "        for jj in sarr:\n",
    "            nn = np.where((np.asarray(dataspec['class']) == ii) & \n",
    "                            (np.asarray(dataspec['type']) ==jj ))[0]\n",
    "            if len(nn) == 0:\n",
    "                print 'no %s in %s'% (ii, jj)\n",
    "                nn = np.where(np.asarray(dataspec['class']) == ii\n",
    "                            )[0]\n",
    "            \n",
    "            ichoice= np.random.choice(nn)\n",
    "            print ichoice\n",
    "            sortz.append(ichoice)\n",
    "        \n",
    "    for i0 in range(nxm):\n",
    "        i = sortz[i0]\n",
    "        ax= plt.subplot(gs[ix,iy])\n",
    "        idg = ix + grid[0]*iy\n",
    "        for fname in filternames:\n",
    "            filt = jplus.tools.fetch_jplus_filter(fname, quiet=True)\n",
    "    #        print np.median(filt.wave)\n",
    "            ww = [filt.avgwave(), filt.avgwave()]\n",
    "            ff = [dataspec[fname][i,0], dataspec[fname][i,0]]\n",
    "\n",
    "            ax.plot(ww,ff,'o', color='blue', markersize=10)\n",
    "            ax.errorbar(ww,ff,yerr= dataspec[fname][i,1], color='blue')\n",
    "\n",
    "            if fname == 'J0660':# or fname == 'J0378' or fname == 'J0395' or fname=='J0861':\n",
    "                fwidth = filt.rectwidth()\n",
    "                ax.fill_between([ww[0] - fwidth/2.,ww[0] + fwidth/2.,ww[0] + fwidth/2.,ww[0] - fwidth/2.],\n",
    "                            [17,17, 28, 28],facecolor='Grey',alpha=0.2)\n",
    "            #    mf = np.max(filt.throughput)\n",
    "            #    ax.fill_between(filt.wave,-25 + (filt.throughput/mf*21)\n",
    "            #   ,facecolor='Grey',alpha=0.2)\n",
    "        fw = 0   \n",
    "        if sdss_fnames[0] in dataspec: # if sdss bands exist\n",
    "            for fname in sdss_fnames:\n",
    "                filt = jplus.tools.fetch_sdss_filter(fname)\n",
    "    #        ww = [filt[0,filt[1,:].argmax()],filt[0,filt[1,:].argmax()]]\n",
    "                ww = [sdss_mw[fw], sdss_mw[fw]]\n",
    "                fw += 1\n",
    "                ff = [dataspec[fname][i,0], dataspec[fname][i,0]]\n",
    "                ax.plot(ww,ff,'o', color='red', markersize=10)\n",
    "                ax.errorbar(ww,ff,yerr= dataspec[fname][i,1], color='red')\n",
    "\n",
    "        ax.text(0.3,0.9,r'$z_{\\rm spec}=%.2f$'%dataspec['z_spec'][i], transform= ax.transAxes, fontsize=12)\n",
    "        ax.text(0.3,0.8,r'$\\Delta m=%.2f$'%dataspec['dm_j0660'][i], transform= ax.transAxes, fontsize=12, color='red')\n",
    "        ax.text(0.3,0.7,dataspec['type'][i], transform= ax.transAxes, fontsize=12, color='black')\n",
    "        ax.text(0.1,0.9,'(%d)'%idg, transform=ax.transAxes,fontsize=15)\n",
    "        ax.set_ylim([25, 17.99])\n",
    "        ax.set_xlim([3000, 8999])\n",
    "        if ix != grid[0]-1:\n",
    "            ax.set_xticklabels([])\n",
    "        if iy >0:\n",
    "            ax.set_yticklabels([])\n",
    "        iy +=1\n",
    "        if iy == grid[1]:\n",
    "            ax.text(1.1,0.5,ssarr[ix],transform=ax.transAxes, fontsize=20,rotation=-90)\n",
    "            iy = 0\n",
    "            ix += 1\n",
    "            \n",
    "\n",
    "    plt.show()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of some features for all classes\n",
    "suff = 'SDSS'\n",
    "snr = 1./dataspec['r'+suff][:,1]\n",
    "snr_ha = 1./dataspec['J0660'][:,1]\n",
    "\n",
    "#classes_names = ['Halpha', 'OIII', 'Hbeta','OII']\n",
    "classes_names = ['Halpha', 'OIII+Hbeta','OII', 'contaminant']\n",
    "colors = plt.cm.coolwarm(np.linspace(0,1,len(classes_names)))\n",
    "iic = 0\n",
    "plt.figure(1,figsize=(10,3))\n",
    "\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(snr[mask], bins=20, color=colors[iic], alpha=0.5, label=ic, range=[2,200])\n",
    "    iic += 1\n",
    "    \n",
    "plt.xlabel('r SNR',fontsize=20)\n",
    "plt.xlim([0,200])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(2,figsize=(10,4))\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(snr_ha[mask], bins=20, color=colors[iic], alpha=0.5, label=ic, range=[2,100])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel('J0660 SNR',fontsize=20)\n",
    "plt.xlim([0,100])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "\n",
    "plt.figure(3,figsize=(10,4))\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['mu_max_r'][mask]-dataspec['rJAVA'][mask,0], bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$\\mu_{\\rm max}- rJAVA$',fontsize=20)\n",
    "#plt.xlim([0,100])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "#plt.figure(4,figsize=(10,4))\n",
    "#suff = 'SDSS'\n",
    "\n",
    "#iic= 0\n",
    "#for ic in classes_names:\n",
    "#    mask = np.asarray(dataspec['class']) == ic\n",
    "#    plt.hist(dataspec['dm_j0660'][mask], bins=20, color=colors[iic], alpha=0.5, label=ic,range=[0,2])\n",
    "#    iic += 1\n",
    "\n",
    "#plt.xlabel(r'$C^{r,g} - J0660$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "#plt.legend()\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(5,figsize=(10,3))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['g'+suff][mask,0] - dataspec['r'+suff][mask,0], bins=20, color=colors[iic], alpha=0.5, label=ic)#,range=[0,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$g-r$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(6,figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['u'+suff][mask,0] - dataspec['z'+suff][mask,0], bins=20, color=colors[iic], alpha=0.5, label=ic)#,range=[0,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$u-z$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(7,figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0861'][mask,0] - dataspec['z'+suff][mask,0], bins=20, color=colors[iic], alpha=0.5, label=ic,range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0861-z$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(8,figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    right_col = dataspec['r'+suff][mask,0]-dataspec['J0660'][mask,0]\n",
    "    left_col = dataspec['g'+suff][mask,0]-dataspec['J0660'][mask,0]\n",
    "    plt.hist(right_col/left_col, bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$(r-J0660)/(g-J0660)$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0395-g',figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0395'][mask,0] - dataspec['g'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0395-g$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0410-g',figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0410'][mask,0] - dataspec['g'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0410-g$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0430-g',figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0430'][mask,0] - dataspec['g'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0430-g$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0515-g',figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0515'][mask,0] - dataspec['g'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0515-g$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0378-u',figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0378'][mask,0] - dataspec['u'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0378-u$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0395-u',figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0395'][mask,0] - dataspec['u'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0395-u$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "\n",
    "\n",
    "bb = ['u','g','r','i','z']\n",
    "\n",
    "for band in bb:\n",
    "    plt.figure(band,figsize=(10,4))\n",
    "\n",
    "\n",
    "    iic= 0\n",
    "    for ic in classes_names:\n",
    "        mask = np.asarray(dataspec['class']) == ic\n",
    "        plt.hist(dataspec[band+suff][mask,0] , bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "        iic += 1\n",
    "\n",
    "    plt.xlabel(r'$%s%s$'%(band,suff),fontsize=20)\n",
    "    #plt.xlim([-0.5,2])\n",
    "    plt.legend()\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "\n",
    "\n",
    "\n",
    "pzarr = ['leph', 'tpz', 'bpz']    \n",
    "\n",
    "for pz in pzarr:\n",
    "    plt.figure(pz,figsize=(10,4))\n",
    "\n",
    "\n",
    "    iic= 0\n",
    "    for ic in classes_names:\n",
    "        mask = np.asarray(dataspec['class']) == ic\n",
    "        plt.hist(dataspec['photoz_'+pz][mask], bins=20, color=colors[iic], alpha=0.5, label=ic, range=[0,1])\n",
    "        iic += 1\n",
    "\n",
    "    plt.xlabel(r'$z_{\\rm %s}$'%pz,fontsize=20)\n",
    "    #plt.xlim([-0.5,2])\n",
    "    plt.legend()\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "\n",
    "\n",
    "    \n",
    "plt.figure('dm_j0660',figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['dm_j0660'][mask], bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$C - J0660$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('dm_j0660_err',figsize=(10,4))\n",
    "\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['dm_j0660_err'][mask], bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$err(C - J0660)$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('cc1')\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "iic =0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    if ic == 'contaminant':\n",
    "        counts, ybins, xbins, image = plt.hist2d(dataspec['r'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             dataspec['i'+suff][mask,0] - dataspec['J0660'][mask,0], bins=80, \n",
    "                                         cmap=plt.cm.Reds, normed=LogNorm(), \n",
    "                                         cmin=0.01, label='SDSS contaminants', alpha=0.7)\n",
    "        plt.legend(loc='upper left')\n",
    "    else:\n",
    "        plt.plot(dataspec['r'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             dataspec['i'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             'o',color=colors[iic],alpha=1,label=ic,markersize=15)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$rSDSS-J0660$',fontsize=20)\n",
    "plt.ylabel(r'$iSDSS-J0660$',fontsize=20)\n",
    "\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "             \n",
    "plt.figure('cc2')\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "xr = [-.5,1.5]\n",
    "yr = [-1,3.5]\n",
    "\n",
    "iic =0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    if ic == 'contaminant':\n",
    "        counts, ybins, xbins, image = plt.hist2d(dataspec['r'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             dataspec['g'+suff][mask,0] - dataspec['J0660'][mask,0], bins=80, \n",
    "                                         cmap=plt.cm.Reds, normed=LogNorm(), \n",
    "                                         cmin=0.01, label='SDSS contaminants', alpha=0.7,\n",
    "                                                range=[xr,yr])\n",
    "        plt.legend(loc='upper left')\n",
    "    else:\n",
    "        plt.plot(dataspec['r'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             dataspec['g'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             'o',color=colors[iic],alpha=1,label=ic,markersize=15)\n",
    "    iic += 1\n",
    "plt.ylim(yr)\n",
    "plt.xlim(xr)\n",
    "plt.xlabel(r'$r%s-J0660$'%suff,fontsize=20)\n",
    "plt.ylabel(r'$g%s-J0660$'%suff,fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading synthetic photometry from various surveys\n",
    "import learn_elgs as learn\n",
    "reload(learn)\n",
    "tfout = '/home/CEFCA/aaorsi/work/elg_jplus/trainspec.dat'\n",
    "vvds = '/home/CEFCA/aaorsi/work/elg_jplus/trainspec_vvds.dat'\n",
    "\n",
    "# load photo_spectra:\n",
    "psfile = '%s/%s'%(DataDir, 'data_spec.data')\n",
    "\n",
    "ps_dict = pickle.load(open(psfile))\n",
    "\n",
    "alleboss     = ps_dict['alleboss']\n",
    "photo_eboss  = ps_dict['photo_eboss']\n",
    "allvvds      = ps_dict['allvvds']\n",
    "photo_vvds   = ps_dict['photo_vvds']\n",
    "\n",
    "allspec = alleboss + allvvds\n",
    "\n",
    "photo_spec = {}\n",
    "nvvds = len(photo_vvds['uJAVA'])\n",
    "for key in photo_eboss.keys():\n",
    "    photo_spec[key] = photo_eboss[key].tolist()\n",
    "    for j in range(nvvds):\n",
    "        photo_spec[key].append(photo_vvds[key][j])\n",
    "    photo_spec[key] = np.asarray(photo_spec[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing on synthetic photometry\n",
    "apply_mask = True\n",
    "synth_spec = []\n",
    "\n",
    "\n",
    "suff ='SDSS'\n",
    "photo_spec['dm_j0660'], Fline, Fcont = mtools.gen_3fm(photo_spec['J0660'][:,0],  \n",
    "                    photo_spec['r'+suff][:,0], photo_spec['g'+suff][:,0], \n",
    "                    Broad_NoLineName='g'+suff)\n",
    "\n",
    "\n",
    "\n",
    "mask = photo_spec['dm_j0660'] > 0.1\n",
    "\n",
    "eboss_id = []\n",
    "vvds_id  = []\n",
    "nspec = len(allspec)\n",
    "print allspec[0].keys()\n",
    "for i in range(nspec):\n",
    "    synth_spec.append(i)\n",
    "    if allspec[i]['survey'] == 'eBOSS':\n",
    "        eboss_id.append(i)\n",
    "    elif allspec[i]['survey'] == 'VVDS-F02_UDEEP':\n",
    "        vvds_id.append(i)\n",
    "    else:\n",
    "        print allspec[i]['survey']\n",
    "        \n",
    "print len(eboss_id), len(vvds_id)\n",
    "        \n",
    "gtype = ['OIII', 'OII', 'Hbeta']\n",
    "gtdict = {}\n",
    "\n",
    "for gt in gtype:\n",
    "    gtdict[gt] = []\n",
    "    for sp in synth_spec:\n",
    "        if allspec[sp]['name'] == gt:\n",
    "            if mask[sp]:\n",
    "                gtdict[gt].append(sp)\n",
    "\n",
    "for gtype in gtdict.keys():\n",
    "    print 'Number of %s: %d'% (gtype, len(gtdict[gtype]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print photo_spec.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore those colour-colour plots again now with synthetic photometry and adding\n",
    "# cuts to reduce the fraction of contaminants\n",
    "\n",
    "classes_names = ['OIII+Hbeta','OII', 'contaminant']\n",
    "\n",
    "plt.figure('ccx1',figsize=(8,8))\n",
    "colors = plt.cm.coolwarm(np.linspace(0,1,len(classes_names)))\n",
    "colors_synt = plt.cm.Accent(np.linspace(0,1,3))\n",
    "suff = 'SDSS'\n",
    "gtype = ['OIII', 'OII', 'Hbeta']\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "iic =0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    if ic == 'contaminant':\n",
    "        counts, ybins, xbins, image = plt.hist2d(dataspec['r'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             dataspec['i'+suff][mask,0] - dataspec['J0660'][mask,0], bins=80, \n",
    "                                         cmap=plt.cm.Reds, normed=LogNorm(), \n",
    "                                         cmin=0.01, label='SDSS contaminants', alpha=0.7)\n",
    "        #plt.legend(loc='upper left')\n",
    "    else:\n",
    "        plt.plot(dataspec['r'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             dataspec['i'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             'o',color=colors[iic],alpha=1,label=ic,markersize=15)\n",
    "    iic += 1\n",
    "\n",
    "# Add synthetic photometry from eBOSS\n",
    "iic = 0\n",
    "for ic in gtype:\n",
    "    iid = gtdict[ic]\n",
    "    plt.plot(photo_spec['r'+suff][iid] - photo_spec['J0660'][iid], \n",
    "             photo_spec['i'+suff][iid] - photo_spec['J0660'][iid],\n",
    "             '.',color=colors_synt[iic],alpha=1,label='Synthetic %s'%ic,markersize=15)\n",
    "\n",
    "    iic += 1\n",
    "    \n",
    "plt.xlabel(r'$r%s-J0660$'%suff,fontsize=20)\n",
    "plt.ylabel(r'$i%s-J0660$'%suff,fontsize=20)\n",
    "\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "             \n",
    "plt.figure('cc2',figsize=(8,8))\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "xr = [-.5,1.5]\n",
    "yr = [-1,3.5]\n",
    "\n",
    "iic =0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    if ic == 'contaminant':\n",
    "        counts, ybins, xbins, image = plt.hist2d(dataspec['r'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             dataspec['g'+suff][mask,0] - dataspec['J0660'][mask,0], bins=80, \n",
    "                                         cmap=plt.cm.Reds, normed=LogNorm(), \n",
    "                                         cmin=0.01, label='SDSS contaminants', alpha=0.7,\n",
    "                                                range=[xr,yr])\n",
    "        #plt.legend(loc='upper left')\n",
    "    else:\n",
    "        plt.plot(dataspec['r'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             dataspec['g'+suff][mask,0] - dataspec['J0660'][mask,0],\n",
    "             'o',color=colors[iic],alpha=1,label=ic,markersize=15)\n",
    "    iic += 1\n",
    "\n",
    "iic = 0\n",
    "for ic in gtype:\n",
    "    iid = gtdict[ic]\n",
    "    plt.plot(photo_spec['r'+suff][iid] - photo_spec['J0660'][iid], \n",
    "             photo_spec['g'+suff][iid] - photo_spec['J0660'][iid],\n",
    "             '.',color=colors_synt[iic],alpha=1,label='Synthetic %s'%ic,markersize=15)\n",
    "\n",
    "    iic += 1\n",
    "\n",
    "\n",
    "    \n",
    "plt.ylim(yr)\n",
    "plt.xlim(xr)\n",
    "plt.xlabel(r'$r%s-J0660$'%suff,fontsize=20)\n",
    "plt.ylabel(r'$g%s-J0660$'%suff,fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional cuts on training set to reduce contaminants\n",
    "suff ='SDSS'\n",
    "gj0660 = 2.0\n",
    "mask2 = (dataspec['g'+suff][:,0] - dataspec['J0660'][:,0]) < gj0660\n",
    "\n",
    "#dataspec_new = \n",
    "\n",
    "print 'old fraction of contaminants: %.3f'% ((0.0+len(np.where(np.asarray(dataspec['class']) == 'contaminant')[0]))/\n",
    "                                             (0.0+len(dataspec['class'])))\n",
    "print 'new fraction: %.3f'% ((0.0+len(np.where(np.array(mask2))[0]))/len(dataspec['class']))\n",
    "\n",
    "print len(np.where(np.array(mask2))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will add synthetic spectra to the dataset for training and testing\n",
    "Add_Synthetic_to_DataSet = True\n",
    "\n",
    "data_merged = {}\n",
    "\n",
    "keys_to_copy = photo_spec.keys()\n",
    "mask_sp = photo_spec['g'+suff][:,0] - photo_spec['J0660'][:,0] < gj0660\n",
    "\n",
    "print gtype\n",
    "\n",
    "for key in dataspec.keys():\n",
    "    if type(dataspec[key]) is np.ndarray:\n",
    "        data_merged[key] = dataspec[key][mask2].tolist() # first copy all dataspec into merged dataset\n",
    "    else:\n",
    "        data_merged[key] = np.array(dataspec[key])[mask2].tolist()\n",
    "        \n",
    "for gt in gtype: #for each new class\n",
    "    iid = gtdict[gt]\n",
    "    cname = 'OIII+Hbeta' if gt == 'Hbeta' or gt == 'OIII' else 'OII'\n",
    "    for _i in iid: # and each new object\n",
    "        if mask_sp[_i]: # if object is not masked\n",
    "            for key in keys_to_copy: #append on each key\n",
    "                data_merged[key].append(photo_spec[key][_i])\n",
    "            data_merged['class'].append(cname)\n",
    "\n",
    "data_merged['index'] = np.arange(len(data_merged['class']))\n",
    "data_merged['class'] = np.array(data_merged['class']) \n",
    "\n",
    "print 'converting lists to arrays...'\n",
    "for key in keys_to_copy:\n",
    "    data_merged[key] = np.array(data_merged[key])\n",
    "\n",
    "\n",
    "m_null = np.asarray(data_merged['class']) == 'contaminant'\n",
    "m_z0  = np.asarray(data_merged['class']) == 'Halpha'\n",
    "m_zp3 = np.asarray(data_merged['class']) == 'OIII+Hbeta'\n",
    "#m_zp35 = np.asarray(dataspec['class']) == 'Hbeta'\n",
    "m_zp7 = np.asarray(data_merged['class']) == 'OII'\n",
    "\n",
    "print m_null\n",
    "print data_merged['class']\n",
    "n_null = len(data_merged['class'][m_null])\n",
    "nz0  = len(data_merged['class'][m_z0])\n",
    "nzp3 = len(data_merged['class'][m_zp3])  \n",
    "#nzp35 = len(dataspec['z_spec'][m_zp35])  \n",
    "nzp7 = len(data_merged['class'][m_zp7])\n",
    "print 'Number of Halpha emitters: %ld' % nz0\n",
    "print 'Number of OIII+Hbeta emitters: %ld' % nzp3       \n",
    "#print 'Number of Hbeta emitters: %ld' % nzp35\n",
    "print 'Number of OII emitters: %ld' % nzp7\n",
    "print 'Number of contaminants: %ld' % n_null\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function retrieves a features list for a single object.\n",
    "\n",
    "def prepare_sample(data, index, sample_amp = False, \n",
    "                   sample_error = False, sample_type = 'All_Mags', \n",
    "                   filters='J-PLUS'):\n",
    "    \n",
    "    BBNames = ['u','g','r','i','z']\n",
    "    NBNames = ['J0378', 'J0395', 'J0410','J0430','J0515','J0660','J0861']\n",
    "    \n",
    "    suff = 'JAVA' if filters == 'J-PLUS' else 'SDSS'\n",
    "    \n",
    "    filternames = [ 'J0378','J0395', 'J0410','J0430','J0515', 'J0660',\n",
    "                   'J0861','u'+suff, 'g'+suff, 'r'+suff, 'i'+suff,'z'+suff]\n",
    "        \n",
    "        \n",
    "    \n",
    "    flist = [] # filterslist\n",
    "    ferrlist = []\n",
    "    nfilters = len(filternames)\n",
    "    a_r = data['r'+suff][index,0]\n",
    "    amplitude = np.random.uniform(18, 23) if sample_amp else a_r  # amplitude scaling change for r-filter\n",
    "    DeltaM = amplitude - a_r\n",
    "    for ff in filternames:\n",
    "        if sample_error and data[ff][index,1] > 0:\n",
    "            flist.append(np.random.normal(data[ff][index,0], data[ff][index,1]) + DeltaM)  # errors distribute like a gaussian (?)\n",
    "        else:\n",
    "            flist.append(data[ff][index,0] + DeltaM)\n",
    "        ferrlist.append(data[ff][index,1])\n",
    "    \n",
    "    sample = [] # The sample's features\n",
    "    \n",
    "#    dm_J0515 = mtools.gen_3fm(data['J0515'][index,0], data['g'+suff][index,0], \n",
    "#                          data['r'+suff][index,0],Broad_NoLineName='rSDSS', LineFilterName='J0515', \n",
    "#                          Broad_LineName='gSDSS')\n",
    "#    err_dm_J0515 = mtools.gen_3fm_err(data['J0515'][index,0], data['J0515'][index,1], data['g'+suff][index,0], \n",
    "#                                      data['g'+suff][index,1], data['r'+suff][index,0], data['r'+suff][index,1],\n",
    "#                                      Broad_NoLineName='rSDSS')\n",
    "\n",
    "\n",
    "#    dm_J0378 = mtools.gen_3fm(data['J0378'][index,0], data['u'+suff][index,0], \n",
    "#                          data['g'+suff][index,0],Broad_NoLineName='gSDSS', LineFilterName='J0378', \n",
    "#                              Broad_LineName='uJAVA')\n",
    "##    err_dm_J0378 = mtools.gen_3fm_err(data['J0378'][index,0], data['J0378'][index,1], data['u'+suff][index,0], \n",
    "#                                      data['u'+suff][index,1],data['g'+suff][index,0], data['g'+suff][index,1],\n",
    "#                                      Broad_NoLineName='gSDSS')\n",
    "\n",
    "#    dm_J0861 = mtools.gen_3fm(data['J0861'][index,0], data['z'+suff][index,0], \n",
    "#                          data['i'+suff][index,0],Broad_NoLineName='iSDSS', \n",
    "#                          LineFilterName='J0861', Broad_LineName='zSDSS')\n",
    "\n",
    "#    err_dm_J0861 = mtools.gen_3fm_err(data['J0861'][index,0], data['J0861'][index,1], data['z'+suff][index,0], \n",
    "#                          data['z'+suff][index,1],data['i'+suff][index,0], data['i'+suff][index,1],\n",
    "#                          Broad_NoLineName='iSDSS')\n",
    "\n",
    "#    dm_J0660 = mtools.gen_3fm(data['J0660'][index,0], data['r'+suff][index,0], \n",
    "#                          data['g'+suff][index,0],Broad_NoLineName='gSDSS', \n",
    "#                          LineFilterName='J0660', Broad_LineName='rSDSS')\n",
    "\n",
    "    dm_J0660 = data['dm_j0660'][index]\n",
    "\n",
    "#    err_dm_J0660 = mtools.gen_3fm_err(data['J0660'][index,0], data['J0660'][index,1], data['r'+suff][index,0], \n",
    "#                          data['r'+suff][index,1],data['g'+suff][index,0], data['g'+suff][index,1],\n",
    "#                          Broad_NoLineName='gSDSS')\n",
    "\n",
    "    \n",
    "    # Here I should get creative\n",
    "    if sample_type == 'All_Mags':\n",
    "        sample = flist # All individual filters\n",
    "    if sample_type == 'All_Mags_mumax':\n",
    "        for i in range(nfilters):\n",
    "            sample.append(flist[i])\n",
    "        sample.append(data['mu_max_r'][index] - data['rJAVA'][index,0])\n",
    "        #sample.append(data['cstar'][index])\n",
    "#        sample.append(data['pz_bpz'][index])\n",
    "    if sample_type == 'Colors':\n",
    "        for i in range(nfilters):\n",
    "            sample.append(flist[i])\n",
    "            for j in range(nfilters):\n",
    "                if i != j:\n",
    "                    sample.append(flist[i] - flist[j])\n",
    "                   \n",
    "    if sample_type == 'All':\n",
    "        for i in range(nfilters):\n",
    "            sample.append(flist[i])\n",
    "        sample.append(data['mu_max_r'][index])\n",
    "        #sample.append(data['cstar'][index])\n",
    "        sample.append(data['pz_bpz'][index])\n",
    "        for i in range(nfilters):\n",
    "            sample.append(flist[i])\n",
    "            for j in range(nfilters):\n",
    "                if i != j:\n",
    "                    sample.append(flist[i] - flist[j]) \n",
    "    if sample_type == 'dm':\n",
    "        # Delta-m and SNR of deltaMs...\n",
    "        sample=[#dm_J0515, #1./err_dm_J0515,\n",
    "                #dm_J0378, #1./err_dm_J0378,\n",
    "                #dm_J0861#, 1./err_dm_J0861\n",
    "               dm_J0660]#, err_dm_J0660 ]\n",
    "                 #data['dm'][index],data['err_dm'][index]\n",
    "                 #]\n",
    "        #sample.append(data['mu_max_r'][index] - data['rJAVA'][index,0])\n",
    "        \n",
    "    \n",
    "        for fff in BBNames:\n",
    "            sample.append(np.random.normal(data[fff+suff][index,0], data[fff+suff][index,1]) + DeltaM\n",
    "            if sample_error and data[fff+suff][index,1] > 0 else data[fff+suff][index,0] + DeltaM)\n",
    "        \n",
    "        for nb in NBNames:\n",
    "            sample.append(np.random.normal(data[nb][index,0], data[nb][index,1]) + DeltaM\n",
    "            if sample_error and data[nb][index,1] > 0 else data[nb][index,0] + DeltaM)\n",
    "\n",
    "        \n",
    "        #u-g\n",
    "        sample.append(np.random.normal(data['u'+suff][index,0], data['u'+suff][index,1])-\n",
    "                      np.random.normal(data['g'+suff][index,0], data['g'+suff][index,1])\n",
    "                      if sample_error and data['g'+suff][index,1] > 1 else \n",
    "                      data['u'+suff][index,0]-data['g'+suff][index,0])\n",
    "        \n",
    "        #g-r\n",
    "        sample.append(np.random.normal(data['g'+suff][index,0], data['g'+suff][index,1])-\n",
    "                      np.random.normal(data['r'+suff][index,0], data['r'+suff][index,1])\n",
    "                      if sample_error and data['g'+suff][index,1] > 0 else \n",
    "                      data['g'+suff][index,0]-data['r'+suff][index,0])\n",
    "\n",
    "        #r-i\n",
    "        sample.append(np.random.normal(data['r'+suff][index,0], data['r'+suff][index,1])-\n",
    "                      np.random.normal(data['i'+suff][index,0], data['i'+suff][index,1])\n",
    "                      if sample_error and data['r'+suff][index,1] > 0 \n",
    "                      else data['r'+suff][index,0]-data['i'+suff][index,0])\n",
    "\n",
    "        #i-z\n",
    "        sample.append(np.random.normal(data['i'+suff][index,0], data['i'+suff][index,1])-\n",
    "                      np.random.normal(data['z'+suff][index,0], data['z'+suff][index,1])\n",
    "                      if sample_error and data['r'+suff][index,1] > 0\n",
    "                      else data['i'+suff][index,0]-data['z'+suff][index,0])\n",
    "\n",
    "        #u-z\n",
    "        sample.append(np.random.normal(data['u'+suff][index,0], data['u'+suff][index,1])-\n",
    "                    np.random.normal(data['z'+suff][index,0], data['z'+suff][index,1])\n",
    "                      if sample_error and data['r'+suff][index,1] > 0 else \n",
    "                      data['u'+suff][index,0]-data['z'+suff][index,0])\n",
    "        \n",
    "        #r-z\n",
    "        sample.append(np.random.normal(data['r'+suff][index,0], data['r'+suff][index,1])-\n",
    "                     np.random.normal(data['z'+suff][index,0], data['z'+suff][index,1])\n",
    "                      if sample_error  and data['r'+suff][index,1] > 0\n",
    "                      else data['r'+suff][index,0]-data['z'+suff][index,0])\n",
    "        \n",
    "        \n",
    "        # colors including NBs\n",
    "        \n",
    "        #\n",
    "        \n",
    "        \n",
    "        \n",
    "        #SNR in r-band\n",
    "        #sample.append(1./data['r'+suff][index,1])\n",
    "        #SNR in J0660 band\n",
    "        #sample.append(1./data['J0660'][index,1])\n",
    "        \n",
    "        # r-band\n",
    "        #sample.append(np.random.normal(data['r'+suff][index,0] if sample_error \n",
    "        #                            and data['r'+suff][index,1] > 0 else data['r'+suff][index,0]))\n",
    "        #g-band\n",
    "        #sample.append(np.random.normal(data['g'+suff][index,0] if sample_error else data['g'+suff][index,0]))\n",
    "        \n",
    "        \n",
    "        \n",
    "    return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = data_merged  #use dataspec for xmatches only, data_merged to include synthetic photometry \n",
    "\n",
    "\n",
    "Compute_Dataset = True   # if False it reads a dataset from a file\n",
    "\n",
    "Training_set_class_frac = 1 # a fraction X of the less abundant determines the number of objects per class in the training set.\n",
    "Sampling_errors         = 2  # Each training galaxy is resampled X times using its errors\n",
    "\n",
    "\n",
    "# Set only one of these to True\n",
    "Sample_Small_Only = False       # If True, it will resample only the class with the smallest sample\n",
    "Sample_Balancing  = True        # All classes are resampled but the end result is balanced\n",
    "UseFixedNum       = False       # Use a fixed number of objects per class\n",
    "\n",
    "# indices of different categories of objects\n",
    "id_z0   = dset['index'][m_z0] \n",
    "id_z0p3 = dset['index'][m_zp3]\n",
    "#id_z0p35 = dataspec['index'][m_zp35]\n",
    "id_z0p7 = dset['index'][m_zp7]\n",
    "id_null = dset['index'][m_null]\n",
    "\n",
    "#nzarr = [nz0, nzp3, nzp35, nzp7]\n",
    "nzarr = [nzp3, nzp7, n_null]\n",
    "min_class = np.min(nzarr)\n",
    "id_min    = np.argmin(nzarr)\n",
    "id_max    = np.argmax(nzarr)\n",
    "\n",
    "MaxClass = Sampling_errors * nzarr[id_max] \n",
    "\n",
    "if Sample_Balancing:\n",
    "    print 'MaxClass = %d'%MaxClass\n",
    "\n",
    "    balance_factors= [int((MaxClass+0.)/i) for i in nzarr]\n",
    "\n",
    "if UseFixedNum:\n",
    "    numperclass = int(min_class*Training_set_class_frac)\n",
    "    print 'number of objects in training set per class before sampling errors: %d'%numperclass\n",
    "    print 'number of objects left for validation: %d'%(ngals_tot-numperclass*3)\n",
    "    \n",
    "# Randomised lists:\n",
    "ran_idz0 = np.random.permutation(id_z0)\n",
    "ran_idz0p3 = np.random.permutation(id_z0p3)    \n",
    "#ran_idz0p35 = np.random.permutation(id_z0p35)    \n",
    "ran_idz0p7 = np.random.permutation(id_z0p7)\n",
    "ran_id_null = np.random.permutation(id_null)\n",
    "\n",
    "#id_arr = [ran_idz0, ran_idz0p3, ran_idz0p35, ran_idz0p7]\n",
    "id_arr = [ran_idz0p3, ran_idz0p7, ran_id_null]\n",
    "\n",
    "training_features = []\n",
    "training_class    = []\n",
    "\n",
    "dataset_features = []\n",
    "dataset_class    = []\n",
    "\n",
    "validate_features = []\n",
    "validate_class    = []\n",
    "\n",
    "\n",
    "sample_type='dm'\n",
    "filterset = 'SDSS'\n",
    "\n",
    "if Compute_Dataset:\n",
    "\n",
    "    DoSampling = True if Sampling_errors >= 1 else False\n",
    "\n",
    "    iid = 0\n",
    "    for id_obj in id_arr:\n",
    "        nid = numperclass if UseFixedNum else int(len(id_obj))\n",
    "\n",
    "        if Sample_Small_Only:\n",
    "            DoSampling = True if (Sampling_errors >= 1) and (iid == id_min) else False\n",
    "            Serr = 1\n",
    "        else:\n",
    "            Serr = Sampling_errors\n",
    "\n",
    "        if Sample_Balancing:\n",
    "            DoSampling = True\n",
    "            Serr = balance_factors[iid]\n",
    "            print 'Balancing sample %d with resampling %d'% (iid, Serr)\n",
    "            if Serr == 1:\n",
    "                DoSampling = False # do not resample the largest class\n",
    "                \n",
    "        iclass = 0\n",
    "        for i in range(nid):\n",
    "            for j in range(Serr):\n",
    "                dataset_features.append(prepare_sample(dset, id_obj[i], sample_error=DoSampling, sample_amp = DoSampling, \n",
    "                                                        sample_type=sample_type, filters=filterset))\n",
    "                dataset_class.append(dset['class'][id_obj[i]])\n",
    "                iclass += 1\n",
    "        iid += 1\n",
    "        print 'Number of elements in class %d: %d'%(iid, iclass)\n",
    "\n",
    "\n",
    "    if Training_set_class_frac < 1:    # if == 1 then CV is used instead\n",
    "        # randomize dataset:\n",
    "        ndata = len(dataset_class)\n",
    "        idsort = np.random.permutation(ndata) # permutation of indices\n",
    "         \n",
    "        idtr = int(ndata*Training_set_class_frac) #fraction used for training\n",
    "        id_train = idsort[0:idtr]\n",
    "        id_val   = idsort[idtr:]\n",
    "        \n",
    "        print idtr, ndata, len(id_val)\n",
    "        training_features = [dataset_features[_idt] for _idt in id_train]\n",
    "        training_class    = [dataset_class[_idt] for _idt in id_train]\n",
    "        \n",
    "        validate_features  = [dataset_features[_idv] for _idv in id_val]\n",
    "        validate_class    = [dataset_class[_idv] for _idv in id_val]\n",
    "    else:\n",
    "        training_features = dataset_features\n",
    "        training_class = dataset_class\n",
    "        validate_features = dataset_features\n",
    "        validate_class = dataset_class\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    # Save dataset to a file\n",
    "    datatrain = {'tfeatures':training_features,\n",
    "                   'tclass'   :training_class,\n",
    "                   'vfeatures':validate_features,\n",
    "                   'vclass'   :validate_class}\n",
    "\n",
    "    print 'saving file'\n",
    "    print datatrain.keys()\n",
    "    with open('dataset.data','wb') as outfile:\n",
    "        pickle.dump(datatrain,outfile,protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "\n",
    "else:\n",
    "    datatrain = pickle.load(open('dataset.data'))\n",
    "    training_features = datatrain['tfeatures']\n",
    "    training_class    = datatrain['tclass']\n",
    "    validate_features = datatrain['vfeatures']\n",
    "    validate_class    = datatrain['vclass']\n",
    "    \n",
    "\n",
    "print 'number of objects in training set: %ld'%len(training_features)        \n",
    "print 'number of objects in validation set: %ld'%len(validate_features)        \n",
    "\n",
    "#for i in range(len(training_features)):\n",
    "#    print training_features[i], training_class[i]\n",
    "    \n",
    "print 'Number of features: %ld' % len(training_features[0])\n",
    "ntrain = len(training_features)     \n",
    "nfeat = len(training_features[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for ML analysis elsewhere\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#mlcolors[index]\n",
    "def summary_performance_plot(index,precision, recall, fscore, color, \n",
    "                             cnum, ctype, name,  gs, \n",
    "                             figname='summary', ylim = [.0, 1.1], thick= 3,lsize=20):\n",
    "    \n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax.plot(cnum, precision,'o-', label=name, color=color,linewidth=thick)\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks([0,1,2,3])\n",
    "    ax.set_xticklabels(ctype)\n",
    "    ax.set_title('Precision',fontsize=lsize)\n",
    "    ax.set_xlim([-0.25,2.25])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.legend(loc='upper left',fontsize=lsize)\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=lsize)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=lsize)\n",
    "    \n",
    "    ax = plt.subplot(gs[1])\n",
    "    ax.plot(cnum, recall,'o-', label=name, color=color,linewidth=thick)\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks([0,1,2, 3])\n",
    "    ax.set_xticklabels(ctype)\n",
    "    ax.set_title('Recall',fontsize=lsize)\n",
    "    ax.set_xlim([-0.25,2.25])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=lsize)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=lsize)\n",
    "    if index == 0:\n",
    "        ax.text(0.5,1.15, figname, fontsize=lsize, transform=ax.transAxes)\n",
    "    \n",
    "  #  ax.legend(loc='lower left',fontsize=10)\n",
    "    \n",
    "    ax = plt.subplot(gs[2])\n",
    "    ax.plot(cnum, fscore,'o-', label=name, color=color, linewidth=thick)\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks([0,1,2, 3])\n",
    "    ax.set_xticklabels(ctype)\n",
    "    ax.set_title('Fscore',fontsize=lsize)\n",
    "    ax.set_xlim([-0.25,2.25])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=lsize)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=lsize)\n",
    "\n",
    "   # ax.legend(loc='lower left',fontsize=10)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run ML\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import tree\n",
    "from sklearn import svm \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import sklearn\n",
    "print sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First validate the training set\n",
    "UseCV = False # Use Stratified-Cross-validation to optimise the use of the dataset\n",
    "if UseCV:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "if Training_set_class_frac == 1: # Do the splitting automatically instead of defined as above\n",
    "    traf, valf, trac, valc = train_test_split(\n",
    "    training_features, training_class, test_size=0.2, random_state=None)\n",
    "else:\n",
    "    traf = training_features\n",
    "    valf = validation_features\n",
    "    trac = training_class\n",
    "    valc = validation_class\n",
    "    \n",
    "Scaledata = True\n",
    "\n",
    "if Scaledata:\n",
    "    print 'scaling data...',\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(traf)\n",
    "    Traindata = scaler.transform(traf)\n",
    "    Testdata  = scaler.transform(valf)\n",
    "    print 'done'\n",
    "else:\n",
    "    Traindata = traf\n",
    "    Testdata  = valf    \n",
    "\n",
    "classifiers = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nalpha = 10\n",
    "\n",
    "a_arr = np.logspace(-3,0,num=nalpha)\n",
    "niter = 1\n",
    "iter_arr = np.linspace(1000,1000,niter)\n",
    "nff = int(nfeat)\n",
    "for aa in range(nalpha):\n",
    "    for bbin in iter_arr:\n",
    "        alpha = a_arr[aa]\n",
    "        itt = bbin\n",
    "        print alpha\n",
    "        classifiers['MLP alpha=%.7f\\niter=%d'%(alpha,itt)] = MLPClassifier(solver='lbfgs',\n",
    "                                                hidden_layer_sizes=(nff, nff),\n",
    "                                                activation='relu', alpha=alpha,\n",
    "                                                             random_state=3455, tol=1e-5, \n",
    "                                                              max_iter=itt)\n",
    "\n",
    "\n",
    "\n",
    "cnum = [0, 1, 2]\n",
    "#ctype = ['Halpha', 'OIII', 'Hbeta','OII']\n",
    "ctype = ['OIII+Hbeta','OII','contaminant']\n",
    "nclass = 1\n",
    "mlcolors = plt.cm.Accent(np.linspace(0,1,nalpha+niter))\n",
    "\n",
    "#plt.figure('summary')\n",
    "\n",
    "plt.figure('performance')\n",
    "plt.rcParams['figure.figsize'] = 15, 10\n",
    "gs = gsc.GridSpec(1,3)\n",
    "gs.update(wspace=0.0, right=1.5,top=0.6)\n",
    "\n",
    "gs2 = gsc.GridSpec(1,3)\n",
    "gs2.update(wspace=0.0, right=1.5,top=0.6)\n",
    "\n",
    "\n",
    "\n",
    "iid = 0\n",
    "for index in range(nalpha):\n",
    "    for itt in iter_arr:\n",
    "        alpha = a_arr[index]\n",
    "        name = 'MLP alpha=%.7f\\niter=%d'%(alpha,itt)\n",
    "        sname = r'$\\alpha=%.4f$'%alpha\n",
    "        classifier = classifiers[name]\n",
    "        print name\n",
    "#        if UseCV:\n",
    "#            skf = StratifiedKFold(n_splits=5)\n",
    "#            ndata = len(Traindata)\n",
    "#            id_data = np.ones(ndata)\n",
    "#            for train, test in skf.split(Traindata, training_class):\n",
    "            \n",
    "#        else:\n",
    "            \n",
    "        classifier.fit(Traindata, trac)\n",
    "        y_train = classifier.predict(Traindata)\n",
    "        y_test  = classifier.predict(Testdata)\n",
    "\n",
    "        precision, recall, fscore, support = metrics.precision_recall_fscore_support(trac, \n",
    "                                                                                     y_train, labels=ctype)\n",
    "        color = mlcolors[index]\n",
    "        plt.figure('performance')\n",
    "        plt.rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "        summary_performance_plot(index,precision, recall, fscore, color, cnum, ctype, sname,  gs, figname='Training', ylim=[0.7,1.05])\n",
    "\n",
    "        precision, recall, fscore, support = metrics.precision_recall_fscore_support(valc, \n",
    "                                                                                     y_test, labels=ctype)\n",
    "        color = mlcolors[iid]\n",
    "        plt.figure('performance2')\n",
    "        plt.rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "        summary_performance_plot(index,precision, recall, fscore, color, cnum, ctype, sname,  gs2, figname='Validation', ylim=[0.7,1.05])\n",
    "        iid += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the trained algorithm over the full J-PLUS set.\n",
    "filterset = 'SDSS'\n",
    "\n",
    "feat_data = []\n",
    "\n",
    "best_alpha = 0.0022\n",
    "best_iter  = 1000\n",
    "ctype = ['OIII+Hbeta','OII','contaminant']\n",
    "\n",
    "plt.figure('performance')\n",
    "plt.rcParams['figure.figsize'] = 15, 10\n",
    "gs = gsc.GridSpec(1,3)\n",
    "gs.update(wspace=0.2, right=1.5,top=0.6)\n",
    "\n",
    "gs2 = gsc.GridSpec(1,3)\n",
    "gs2.update(wspace=0.0, right=1.5,top=0.6)\n",
    "\n",
    "\n",
    "# Train again a specific classifier(s)\n",
    "\n",
    "classifier = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(nff,nff),\n",
    "                            activation='relu', alpha=best_alpha,\n",
    "                            random_state=3455, tol=1e-5, \n",
    "                            max_iter=best_iter)\n",
    "\n",
    "print 'training and testing again best classifier'\n",
    "classifier.fit(Traindata, trac)\n",
    "y_train = classifier.predict(Traindata)\n",
    "y_test  = classifier.predict(Testdata)\n",
    "\n",
    "\n",
    "# plot performance of this specific classifier\n",
    "\n",
    "\n",
    "precision, recall, fscore, support = metrics.precision_recall_fscore_support(trac, \n",
    "                                                                             y_train, labels=ctype)\n",
    "color = mlcolors[2]\n",
    "plt.figure('performance')\n",
    "plt.rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "summary_performance_plot(0,precision, recall, fscore, 'Red', cnum, ctype, 'Training',  gs, \n",
    "                         figname='', ylim=[0.7,1.05], thick=5)\n",
    "\n",
    "precision, recall, fscore, support = metrics.precision_recall_fscore_support(valc, \n",
    "                                                                             y_test, labels=ctype)\n",
    "#plt.figure('performance2')\n",
    "#plt.rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "summary_performance_plot(0,precision, recall, fscore, 'RoyalBlue', cnum, ctype, 'Validation',  gs, \n",
    "                         figname='', ylim=[0.7,1.05],thick=5)\n",
    "iid += 1\n",
    "\n",
    "print 'preparing J-PLUS data'\n",
    "for i in range(len(dcat['tile_id'])):\n",
    "    feat_data.append(prepare_sample(dcat, i, \n",
    "                    sample_type='dm', filters=filterset))\n",
    "\n",
    "\n",
    "if Scaledata:\n",
    "    print 'scaling features'\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(Traindata)\n",
    "    fdata  = scaler.transform(feat_data)\n",
    "else:\n",
    "    fdata = feat_data\n",
    "\n",
    "print 'running trained classifier'\n",
    "pred = classifier.predict(fdata)\n",
    "print 'done with %d objects'% len(dcat['tile_id'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write classification\n",
    "\n",
    "pred_file = '%s/classification.data'%DataDir\n",
    "pred_dict ={'pred': pred, 'all':dcat}\n",
    "\n",
    "with open(pred_file,'wb') as outfile:\n",
    "        pickle.dump(pred_dict,outfile,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noii = len(np.where(np.asarray(pred) == 'OII')[0])\n",
    "noiiihb = len(np.where(np.asarray(pred) == 'OIII+Hbeta')[0])\n",
    "ncont = len(np.where(np.asarray(pred) == 'contaminant')[0])\n",
    "\n",
    "print 'Number of\\nOII emitters:%d\\nOIII-HB emitters:%d\\nContaminants:%d'%(\n",
    "    noii,noiiihb,ncont)\n",
    "\n",
    "print np.unique(np.asarray(training_class))\n",
    "prob = classifier.predict_proba(feat_data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "#import jplus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load databases\n",
    "\n",
    "import pickle\n",
    "dset = pickle.load(open('dataset.data'))\n",
    "print dset.keys()\n",
    "\n",
    "print \n",
    "\n",
    "x_train = np.asarray(dset['tfeatures'])\n",
    "y_train = np.asarray(dset['tclass'])\n",
    "x_test = np.asarray(dset['vfeatures'])\n",
    "y_test    = np.asarray(dset['vclass'])\n",
    "\n",
    "\n",
    "nfeat = len(x_train[0])\n",
    "print nfeat\n",
    "print np.unique(y_train)\n",
    "#blippi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example sets\n",
    "\n",
    "kx_train = np.random.random((1000, 20))\n",
    "ky_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "kx_test = np.random.random((100, 20))\n",
    "ky_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "print np.shape(kx_train), np.shape(x_train)\n",
    "print np.shape(ky_train), np.shape(y_train)\n",
    "\n",
    "def class_to_int(istr):\n",
    "    if istr == 'OII':\n",
    "        return 0\n",
    "    elif istr == 'OIII+Hbeta':\n",
    "        return 1\n",
    "    elif istr == 'contaminant':\n",
    "        return 2\n",
    "    else:\n",
    "        print '%s not recognised'%istr\n",
    "        return -99\n",
    "\n",
    "\n",
    "y_train_int = [class_to_int(x) for x in y_train]\n",
    "y_test_int = [class_to_int(x) for x in y_test]\n",
    "\n",
    "y2_train = keras.utils.to_categorical(y_train_int, num_classes=len(np.unique(y_train_int)))\n",
    "y2_test = keras.utils.to_categorical(y_test_int, num_classes=len(np.unique(y_test_int)))\n",
    "\n",
    "print np.shape(y2_train)\n",
    "\n",
    "print type(y2_train), type(ky_train)\n",
    "\n",
    "print y2_train[0:2] \n",
    "print\n",
    "print ky_train[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras MLP\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(3, activation='relu', input_dim=nfeat))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "print 'doing stuff'\n",
    "\n",
    "print np.shape(x_train)\n",
    "print np.shape(y2_train)\n",
    "\n",
    "model.fit(x_train, y2_train,\n",
    "          epochs=10,\n",
    "          batch_size=200)\n",
    "\n",
    "score = model.evaluate(x_test, y2_test, batch_size=100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print classifier.classes_\n",
    "from astropy.cosmology import Planck15\n",
    "\n",
    "ld= Planck15.luminosity_distance(0.75).value\n",
    "\n",
    "#compute LFs\n",
    "mpc2cm= 3.08e24\n",
    "ld *= mpc2cm\n",
    "id_oii = np.asarray(pred) == 'OII'\n",
    "\n",
    "gal_oii = jplus.tools.select_object(dcat,id_oii)\n",
    "\n",
    "\n",
    "\n",
    "f_oii = gal_oii['F_j0660']\n",
    "\n",
    "loii = f_oii * 4*np.pi*ld**2\n",
    "\n",
    "min_oii = np.min(np.log10(loii))\n",
    "max_oii = np.max(np.log10(loii))\n",
    "lbin = 0.1\n",
    "nbins = int(np.floor((max_oii-min_oii)/lbin+1))\n",
    "print min_oii,max_oii\n",
    "binarr = np.arange(nbins)*lbin + min_oii\n",
    "\n",
    "phi, lbb = np.histogram(np.log10(loii),bins=nbins,range=[min_oii-lbin/2.,max_oii+lbin/2.])\n",
    "\n",
    "depth = Planck15.comoving_distance(0.8) - Planck15.comoving_distance(0.74)\n",
    "area_tile = (1.4*60)**2 #deg^2\n",
    "ntiles = len(np.unique(dcat['tile_id']))\n",
    "print 'ntiles %d'%ntiles\n",
    "mask_factor = 0.8 #to be computed properly\n",
    "area_survey = area_tile*ntiles*mask_factor*(Planck15.kpc_comoving_per_arcmin(0.77).value/(1e3))**2\n",
    "volume = area_survey * depth.value\n",
    "print volume\n",
    "\n",
    "errs =np.sqrt(phi)/(volume * lbin)\n",
    "print errs\n",
    "\n",
    "plt.figure('LFOII',figsize=(8,8))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.semilogy(binarr, phi/volume/lbin,'bo-',linewidth=2,markersize=15,label='J-PLUS',alpha=0.5)\n",
    "ax.errorbar(binarr, phi/volume/lbin, yerr = errs, fmt='b,',alpha=0.5,elinewidth=3)\n",
    "ax.set_ylabel(r'$\\phi ({\\rm d}\\log L)^{-1}[{\\rm Mpc^{-3}]$',fontsize=25)\n",
    "ax.set_xlabel(r'$\\log(L_{OII} [{\\rm erg~s^{-1}}])$',fontsize=25)\n",
    "\n",
    "\n",
    "fmockdata = 'data/z_0.7875'\n",
    "fcomparatdata = 'data/z_0.7875_data'\n",
    "\n",
    "mockphi = np.loadtxt(fmockdata)\n",
    "dataphi = np.loadtxt(fcomparatdata)\n",
    "\n",
    "ax.semilogy(mockphi[:,0],mockphi[:,1],'k--',linewidth=5,label='Mock (Izquierdo+18)')\n",
    "ax.semilogy(dataphi[:,0],dataphi[:,1],'ro',markersize=10,label='Comparat+15',alpha=0.5)\n",
    "ax.errorbar(dataphi[:,0],dataphi[:,1],yerr=[dataphi[:,5],dataphi[:,4]], fmt='r,',alpha=0.7,\n",
    "            elinewidth=3)\n",
    "ax.set_xlim([41,45])\n",
    "ax.set_ylim([5e-9,1e-2])\n",
    "ax.text(0.8,0.7,'z=0.75',transform=ax.transAxes,fontsize=20)\n",
    "ax.legend(loc='upper right',fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymangle\n",
    "import pickle\n",
    "generate_random_mask = True\n",
    "\n",
    "if generate_random_mask:\n",
    "    jpm_file = '/home/CEFCA/aaorsi/work/jplus_masks/jplus_dr1_masks.pol'\n",
    "    print 'reading mangle file'\n",
    "    jplus_mask = pymangle.mangle.Mangle(jpm_file)\n",
    "    print 'generating randoms'\n",
    "    jpran = jplus_mask.genrand(100000)\n",
    "    ran_mask = {'coords':np.transpose([jpran[0], jpran[1]])}\n",
    "else:    \n",
    "    ranfile = 'random_mask.data'\n",
    "    ran_mask = pickle.load(open(ranfile))\n",
    "\n",
    "print ran_mask.keys()\n",
    "tiles = jplus.datasets.fetch_jplus_tile_list(db='dr1',overwrite=False)\n",
    "\n",
    "print tiles.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_scale = 1.40\n",
    "\n",
    "def haversine_dist(ra1, dec1, ra2, dec2):\n",
    "  th1 = np.pi/2. - dec1 * np.pi/180.0\n",
    "  th2 = np.pi/2. - dec2 * np.pi/180.0\n",
    "\n",
    "  ph1 = ra1 * np.pi/180.0\n",
    "  ph2 = ra2 * np.pi/180.0\n",
    "\n",
    "  dph = np.abs(ph1 - ph2)\n",
    "  dth = np.abs(th1 - th2)\n",
    "\n",
    "  harg = np.sin(dph/2)**2 + np.cos(ph1)*np.cos(ph2) * np.sin(dth/2.)**2\n",
    "\n",
    "  return 2 *np.arcsin(np.sqrt(harg)) * 180./np.pi  # Return distance in degrees\n",
    "\n",
    "#Get the mask tiles overlapping with ELGs\n",
    " \n",
    "    \n",
    "Load_rans=True\n",
    "ranfile = 'ran_jplus'\n",
    "if not Load_rans:\n",
    "    print 'finding tile_id of random points...'\n",
    "    nran = len(ran_mask['coords'])\n",
    "    print 'initial number of randoms %ld' % nran\n",
    "    ran_tile_id = np.zeros(nran)\n",
    "\n",
    "    m = []\n",
    "    for i in range(nran):\n",
    "        darr = haversine_dist(ran_mask['coords'][i,0], ran_mask['coords'][i,1], \n",
    "                              tiles['ra'], tiles['dec'])\n",
    "        idt2 = np.argsort(darr)[0]  \n",
    "        ran_tile_id[i] = tiles['ref_tileID'][idt2] \n",
    "        m.append(True if darr[idt2] < 2*tile_scale else False)\n",
    "        if i%10000 == 0:\n",
    "            print float(i)/float(nran)\n",
    "\n",
    "    randict = jplus.tools.select_object(ran_mask, m)\n",
    "    randict['tile_id'] = ran_tile_id[m]\n",
    "\n",
    "    print 'Returning only random mask tiles with overlapping tiles with ELGs'\n",
    "\n",
    "    unique_tiles = reduce(np.intersect1d, (np.unique(dcat['tile_id']), \n",
    "                                           np.unique(randict['tile_id'])))\n",
    "    print 'number of intersecting tiles', len(unique_tiles)\n",
    "\n",
    "    ran_mask   = jplus.tools.select_object(randict, np.array([item in unique_tiles for item in randict['tile_id'] ]))\n",
    "    print 'Final number of random points in mask:%ld'%len(ran_mask['tile_id'])\n",
    "    with open(ranfile,'wb') as outfile:\n",
    "        pickle.dump(ran_mask,outfile,protocol=pickle.HIGHEST_PROTOCOL)   \n",
    "    \n",
    "else:\n",
    "    ran_mask = pickle.load(open(ranfile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_mask.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ran_mask['coords'][0:2], ran_mask['tile_id'][0:2]\n",
    "print gal_oii['coords'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw 2PCF:\n",
    "\n",
    "readw = True\n",
    "\n",
    "import CosmoBolognaLib as cbl\n",
    "cosmology = cbl.Cosmology()\n",
    "\n",
    "ran_mask['redshift'] = np.ones(len(ran_mask['coords']))\n",
    "gal_oii['redshift'] = np.ones(len(gal_oii['coords']))\n",
    "\n",
    "ra_mask = ran_mask['coords'][:,0].astype(float)\n",
    "dec_mask = ran_mask['coords'][:,1].astype(float)\n",
    "\n",
    "if readw is False:\n",
    "    cat_objs = cbl.Catalogue(cbl.EnumTypes._Galaxy_, cbl.EnumTypes._observedCoordinates_, \n",
    "                            gal_oii['coords'][:,0],gal_oii['coords'][:,1],gal_oii['redshift'], cosmology, cbl.EnumTypes._degrees_)\n",
    "\n",
    "    ran_objs = cbl.Catalogue(cbl.EnumTypes._RandomObject_, cbl.EnumTypes._observedCoordinates_,\n",
    "                            ra_mask,dec_mask, \n",
    "                             ran_mask['redshift'], cosmology, cbl.EnumTypes._degrees_)\n",
    "\n",
    "\n",
    "\n",
    "    angMin = 0.01                #// minimum angular separation \n",
    "    angMax = 5.                  #// maximum angular separation \n",
    "    nbins = 20                      #// number of bins\n",
    "    shift = 0.5                  #// shift used to set the bin centre \n",
    "    angularUnits = cbl.EnumTypes._degrees_\n",
    "\n",
    "    twopt = cbl.TwoPointCorrelation1D_angular(cat_objs, ran_objs,cbl.EnumTypes._linear_, angMin, angMax, nbins, shift, \n",
    "                                            angularUnits)\n",
    "    cbl.set_ObjectRegion_SubBoxes(cat_objs,ran_objs,3,3,3)\n",
    "\n",
    "    twopt.measure(cbl.EnumTypes._Jackknife_,'./')\n",
    "    twopt.write('./', 'test');\n",
    "    xx = twopt.xx()\n",
    "    xi1D = twopt.xi1D()\n",
    "    error1D = twopt.error1D()\n",
    "else:\n",
    "    xx, xi1D, error1D = np.loadtxt('test',unpack=True)\n",
    "    \n",
    "#plt.figure(4,figsize=(7,7))\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.errorbar(xx, xx*xi1D, error1D, fmt='o',\n",
    "             color='royalblue', label=r'$%.2f<z<%.2f$'%(0.74,0.8))\n",
    "ax.legend(fontsize=25)\n",
    "ax.set_ylim([0.02,0.15])\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "\n",
    "ax.set_xlabel(r'$\\theta [deg]$',fontsize=30)\n",
    "ax.set_ylabel(r'$\\theta\\omega(\\theta)$',fontsize=30)\n",
    "plt.savefig('w_elgcand.pdf',bbox_inches='tight')\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(5)\n",
    "rMin = 1.0\n",
    "rMax = 20.0\n",
    "piMin = 0.\n",
    "piMax = 0.\n",
    "proj_tpf = cbl.TwoPointCorrelation_projected(cat_objs, ran_objs,cbl.EnumTypes._linear_, rMin, rMax, nbins, shift, \n",
    "                                        angularUnits)\n",
    "\n",
    "cbl.set_ObjectRegion_SubBoxes(cat_objs,ran_objs,3,3,3)\n",
    "proj_tpf.measure(cbl.EnumTypes._Jackknife_,'./')\n",
    "proj_tpf.write('./', 'test')\n",
    "\n",
    "plt.errorbar(proj_tpf.xx(), proj_tpf.xi1D(), proj_tpf.error1D(), fmt='o',color='royalblue', label=\"2pt monopole\")\n",
    "plt.legend()\n",
    "plt.xlabel(r'$r_p$',fontsize=20)\n",
    "plt.ylabel(r'$w_p(r_p)$',fontsize=20)\n",
    "plt.savefig('wp_rp.pdf',bbox_inches='tight')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking systematics\n",
    "print gal_oii.keys()\n",
    "tile, mag, total = jplus.plotting.groupbytile(gal_oii['tile_id'], gal_oii['J0660'][:,0])\n",
    "#plt.figure()\n",
    "#plt.plot(tile, mag,'o')\n",
    "#print tile, total\n",
    "\n",
    "n_avg = np.mean(total)\n",
    "n_navg = total/n_avg\n",
    "tile_oii, fwhm, total = jplus.plotting.groupbytile(gal_oii['tile_id'], gal_oii['fwhm'])\n",
    "\n",
    "#print tiles.keys()\n",
    "#plt.figure()\n",
    "#plt.plot(n_navg, fwhm,'.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def med_property(xprop,yprop,nbins,prange):\n",
    "    xarr = np.linspace(prange[0],prange[1],nbins)\n",
    "    yarr= np.zeros(nbins)\n",
    "    pp = np.zeros(nbins)\n",
    "    bs = xarr[1]-xarr[0]\n",
    "    for i in range(nbins):\n",
    "        ss = np.where((xprop >xarr[i] - bs/2.) & (xprop < xarr[i]+bs/2.))[0]\n",
    "        if len(ss) == 0:\n",
    "            continue\n",
    "        yarr[i] = np.median(yprop[ss])\n",
    "        pp[i] = np.std(yprop[ss])\n",
    "        \n",
    "    \n",
    "    return xarr,yarr,pp\n",
    "\n",
    "        \n",
    "nbins= 15\n",
    "prange = [0.65,1.5]\n",
    "\n",
    "def plot_syst(f_favg, n_navg,nbins=15,prange=[0.65,1.5],\n",
    "              xlabel=r'$FWHM_r/\\langle FWHM_r \\rangle$',\n",
    "             ylabel=r'$n_{\\rm gal}/\\langle n_{\\rm gal}\\rangle$'):\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(f_favg, n_navg,',',color='black')\n",
    "    plt.xlabel(xlabel,fontsize=20)\n",
    "    plt.ylabel(ylabel,fontsize=20)\n",
    "    plt.xlim(prange)\n",
    "    plt.plot(prange,[1,1],'--',linewidth=5,color='black')\n",
    "    xp,yp,pp = med_property(f_favg, n_navg,nbins, prange)\n",
    "    #plt.plot(xp,yp,'o')\n",
    "    plt.errorbar(xp,yp,yerr=pp,fmt='o',color='red')\n",
    "    \n",
    "    return xp,yp,pp\n",
    "\n",
    "avg_fwhm = np.mean(fwhm)\n",
    "f_favg  = fwhm / avg_fwhm\n",
    "\n",
    "xp,yp,pp = plot_syst(f_favg, n_navg)\n",
    "\n",
    "print gal_oii['fwhm'][0]\n",
    "print np.sum(total)\n",
    "print len(gal_oii['tile_id'])\n",
    "\n",
    "#Xmatch of SDSS stars with all J-PLUS\n",
    "d,ind = jplus.tools.crossmatch_angular(gal_jplus['coords'],stars_sdss_spec['coords'],max_distance=3e-4)\n",
    "m = ((d != np.inf))\n",
    "\n",
    "as_spec = jplus.tools.select_object(gal_jplus, m)\n",
    "\n",
    "\n",
    "tile_stars, cstar, total = jplus.plotting.groupbytile(as_spec['tile_id'], as_spec['cstar'])\n",
    "avg_stars = np.mean(total)\n",
    "s_savg  = total / avg_stars\n",
    "\n",
    "n_navg2 = []\n",
    "s_savg2 = []\n",
    "it = 0\n",
    "for it in range(len(tile_stars)):\n",
    "    tt = tile_stars[it]\n",
    "    xx = np.where(tt == tile_oii)[0]\n",
    "    if len(xx) == 0:\n",
    "        print 'tile %d not found'%tt\n",
    "        continue\n",
    "    n_navg2.append(n_navg[xx])\n",
    "    s_savg2.append(s_savg[it])\n",
    "\n",
    "print len(n_navg2), len(s_savg2)\n",
    "\n",
    "xp,yp,pp = plot_syst(np.asarray(s_savg2), np.asarray(n_navg2), \n",
    "                     xlabel=r'$n_{\\rm stars}/\\langle n_{\\rm stars}\\rangle$',\n",
    "                    prange=[0.2,2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get properties from tiles\n",
    "\n",
    "print tiles.keys()\n",
    "\n",
    "\n",
    "oii_tiles= np.unique(gal_oii['tile_id'])\n",
    "gal_oii['depth'] = np.zeros(noii)\n",
    "for tt in oii_tiles:\n",
    "    id_oii = np.where(gal_oii['tile_id'] == tt)[0]\n",
    "    id_tarr = np.where(tiles['ref_tileID'] == tt)[0]\n",
    "    gal_oii['depth'][id_oii] = tiles['depth'][id_tarr]\n",
    "\n",
    "\n",
    "tile_oii, depth_tiles, total = jplus.plotting.groupbytile(gal_oii['tile_id'], gal_oii['depth'])\n",
    "\n",
    "avg_depth = np.mean(depth_tiles)\n",
    "f_favg  = depth_tiles / avg_depth\n",
    "xp,yp,pp = plot_syst(f_favg, n_navg, \n",
    "                    xlabel=r'$depth/\\langle depth\\rangle$',\n",
    "                    prange=[0.96,1.06])\n",
    "\n",
    "\n",
    "\n",
    "gal_oii['d_davg'] = f_favg\n",
    "from scipy import interpolate\n",
    "\n",
    "dn = interpolate.interp1d(xp,yp, bounds_error=False, fill_value=1e-10)\n",
    "\n",
    "ww = 1./(dn(f_favg))\n",
    "ww = ww / np.sum(ww)*float(noii)\n",
    "\n",
    "gal_oii['weight'] = ww\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.hist(gal_oii['weight'],bins=50)\n",
    "\n",
    "# Weighted 2pt\n",
    "\n",
    "readw = False\n",
    "if readw is False:\n",
    "    \n",
    "    #cbl.EnumTypes._Weight_ = gal_oii['weight']\n",
    "    #cat_objs = cbl.Catalogue(cbl.EnumTypes._Galaxy_, cbl.EnumTypes._observedCoordinates_, \n",
    "    #                        gal_oii['coords'][:,0],gal_oii['coords'][:,1],gal_oii['redshift'], cbl.EnumTypes._Weight_,\n",
    "    #                         cosmology, cbl.EnumTypes._degrees_)\n",
    "    cat_objs = cbl.Catalogue(cbl.EnumTypes._Galaxy_, cbl.EnumTypes._observedCoordinates_, \n",
    "                             gal_oii['coords'][:,0],gal_oii['coords'][:,1],gal_oii['redshift'], \n",
    "                             gal_oii['weight'], cosmology, cbl.EnumTypes._degrees_)\n",
    "\n",
    "\n",
    "    \n",
    "    ran_objs = cbl.Catalogue(cbl.EnumTypes._RandomObject_, cbl.EnumTypes._observedCoordinates_,\n",
    "                            ra_mask,dec_mask, ran_mask['redshift'],cosmology, cbl.EnumTypes._degrees_)\n",
    "\n",
    "    \n",
    "\n",
    "    angMin = 0.01                #// minimum angular separation \n",
    "    angMax = 4.                  #// maximum angular separation \n",
    "    nbins = 20                      #// number of bins\n",
    "    shift = 0.5                  #// shift used to set the bin centre \n",
    "    angularUnits = cbl.EnumTypes._degrees_\n",
    "\n",
    "    twopt = cbl.TwoPointCorrelation1D_angular(cat_objs, ran_objs,cbl.EnumTypes._linear_, angMin, angMax, nbins, shift, \n",
    "                                            angularUnits)\n",
    "    cbl.set_ObjectRegion_SubBoxes(cat_objs,ran_objs,3,3,3)\n",
    "\n",
    "    twopt.measure(cbl.EnumTypes._Jackknife_,'./')\n",
    "    twopt.write('./', 'test2');\n",
    "    xxc = np.asarray(twopt.xx())\n",
    "    xi1Dc = np.asarray(twopt.xi1D())\n",
    "    error1Dc = np.asarray(twopt.error1D())\n",
    "else:\n",
    "    xxc, xi1Dc, error1Dc = np.loadtxt('test2',unpack=True)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,figsize=(7,7))\n",
    "ax.errorbar(xx, xx*xi1D, error1D, fmt='o',\n",
    "             color='royalblue', label='no systematics')\n",
    "ax.errorbar(xxc, xxc*xi1Dc, error1Dc, fmt='o',\n",
    "             color='red', label='depth')\n",
    "ax.text(0.7,.2,r'$%.2f<z<%.2f$'%(0.74,0.8),transform=ax.transAxes,fontsize=25)\n",
    "ax.legend(fontsize=25)\n",
    "ax.set_ylim([0.02,0.15])\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "\n",
    "ax.set_xlabel(r'$\\theta [deg]$',fontsize=30)\n",
    "ax.set_ylabel(r'$\\theta\\omega(\\theta)$',fontsize=30)\n",
    "plt.savefig('w_elgcand.pdf',bbox_inches='tight')\n",
    "\n",
    "print xxc, xi1Dc\n",
    "print cbl.EnumTypes._Weight_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#classifier = MLPClassifier(solver='lbfgs')\n",
    "C = 1.0\n",
    "kernel = 1.0 * RBF([1.0,1.0,1.0,1.0,1.0,1.0]) # for GPC\n",
    "\n",
    "classifiers = { 'MLP'                       : MLPClassifier(solver='lbfgs',\n",
    "                                                            hidden_layer_sizes=(nfeat),\n",
    "                                                           activation='logistic', alpha=0.21, random_state=1,\n",
    "                                                           tol=1e-6, max_iter=1000) ,\n",
    "                    'Random Forest'             : RandomForestClassifier(n_estimators=50),\n",
    "                    'SVC'                       : svm.SVC(),\n",
    "#                    'L1 logistic'               : LogisticRegression(C=C, penalty='l1'),\n",
    "#                    'L2 logistic (OvR)'         : LogisticRegression(C=C, penalty='l2'),\n",
    "#                    'L2 logistic (Multinomial)' : LogisticRegression(C=C, solver='lbfgs',multi_class='multinomial')\n",
    "#                    'GPC'                       : GaussianProcessClassifier(kernel)\n",
    "                    }\n",
    "\n",
    "nclass = float(len(classifiers))\n",
    "mlcolors = plt.cm.coolwarm(np.linspace(0,1,nclass))\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "cnum = [0, 1, 2]\n",
    "ctype = ['Halpha', 'OIII+Hbeta','OII']\n",
    "\n",
    "for index, (name, classifier) in enumerate(classifiers.items()):\n",
    "\n",
    "    classifier.fit(Traindata, training_class)\n",
    "    y_pred = classifier.predict(Testdata)\n",
    "\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "    #scores = cross_validate(classifier, Traindata, training_class, scoring=scoring,\n",
    "    #         cv=5, return_train_score=True)\n",
    "    #print scores\n",
    "    \n",
    "    #print 'Metrics for %s'%name\n",
    "    #print metrics.classification_report(validate_class, y_pred, labels=ctype, )\n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(validate_class, y_pred, labels=ctype)\n",
    "#    success_rate = np.zeros(3)\n",
    "\n",
    "#    for i in cnum:\n",
    "#        mm = np.where((real_val == i) & (real_val == pred_val))[0]\n",
    "#        success_rate[i] = np.float(len(mm)) / (len(np.where(real_val == i)[0]))\n",
    "\n",
    "    #plt.figure('m1%d'%index)\n",
    "    #cnf_matrix = confusion_matrix(validate_class, y_pred)\n",
    "    #plot_confusion_matrix(cnf_matrix, classes=ctype,\n",
    "    #                  title='Confusion matrix for %s, without normalization'%name, cmap = plt.cm.Blues)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#    plt.figure('m2%d'%index)\n",
    "#    plot_confusion_matrix(cnf_matrix, classes=ctype, normalize=True,\n",
    "#                      title='Normalized confusion matrix for %s'%name)\n",
    "    \n",
    "#    color = mlcolors[index]\n",
    "    figname = 'comp'\n",
    "    plt.figure(figname)\n",
    "    summary_performance_plot(index,precision, recall, fscore, color, cnum, ctype, name,  gs2, figname='Validation', ylim=[0.2,1.05])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelining: chaining a PCA and a logistic regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(Traindata)\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_')\n",
    "\n",
    "# Prediction\n",
    "n_components = [20, 40, 64]\n",
    "Cs = np.logspace(-4, 4, 3)\n",
    "\n",
    "# Parameters of pipelines can be set using __ separated parameter names:\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(pca__n_components=n_components,\n",
    "                              logistic__C=Cs))\n",
    "estimator.fit(Traindata, training_class)\n",
    "\n",
    "plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "plt.legend(prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(validate_class, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=ctype,\n",
    "                      title='Confusion matrix, without normalization', cmap = plt.cm.Blues)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=ctype, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
