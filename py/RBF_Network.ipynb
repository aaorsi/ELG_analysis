{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I implement Radial Basis Function Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is taken from \n",
    "# http://www.rueckstiess.net/research/snippets/show/72d2363e\n",
    "# and modified subsequently to allow for the Least Squares algorithm to build the hidden layer.\n",
    "\n",
    "#from scipy import *\n",
    "from scipy.linalg import norm, pinv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class RBF:\n",
    "     \n",
    "    def __init__(self, indim, centers, betas, outdim,):\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        numCenters = len(centers)\n",
    "        if numCenters != len(betas):\n",
    "            print 'number of centers and betas must be the same.'\n",
    "#        self.numCenters = numCenters\n",
    "        self.numCenters = numCenters\n",
    "#        self.centers = [random.uniform(-1, 1, indim) for i in xrange(numCenters)]\n",
    "        self.centers = centers\n",
    "        self.beta    = betas\n",
    "#        self.beta = 8\n",
    "        self.W = np.random.rand(self.numCenters, self.outdim)\n",
    "         \n",
    "    def _basisfunc(self, c, d, bval):\n",
    "        assert len(d) == self.indim\n",
    "        return np.exp(-1 * norm(bval*(c-d)**2))\n",
    "      \n",
    "     \n",
    "    def _calcAct(self, X):\n",
    "        # calculate activations of RBFs\n",
    "#        G = np.zeros((np.array(X).shape[0], self.numCenters), float)\n",
    "        G = np.zeros([len(X), self.numCenters])\n",
    "        for ci, c in enumerate(self.centers):\n",
    "            beta_ci = self.beta[ci]\n",
    "            for xi, x in enumerate(X):\n",
    "                G[xi,ci] = self._basisfunc(c, x, beta_ci)\n",
    "        return G\n",
    "     \n",
    "    def train(self, X, Y):\n",
    "        \"\"\" X: matrix of dimensions n x indim \n",
    "            y: column vector of dimension n x 1\n",
    "            idx: indices of training set objects acting as prototypes \"\"\"\n",
    "         \n",
    "        # choose random center vectors from training set\n",
    "        #rnd_idx = random.permutation(X.shape[0])[:self.numCenters]\n",
    "        #self.centers = [X[i,:] for i in rnd_idx]\n",
    "        #self.centers = [X[i,:] for i in idx] \n",
    "        #print \"center\", self.centers\n",
    "        # calculate activations of RBFs\n",
    "        G = self._calcAct(X)\n",
    "        #print G\n",
    "         \n",
    "        # calculate output weights (pseudoinverse)\n",
    "        self.W = np.dot(pinv(G), Y)\n",
    "         \n",
    "    def test(self, X):\n",
    "        \"\"\" X: matrix of dimensions n x indim \"\"\"\n",
    "         \n",
    "        G = self._calcAct(X)\n",
    "        Y = np.dot(G, self.W)\n",
    "        return Y\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rSDSS', 'iSDSS', 'obj', 'gSDSS', 'J0395', 'zSDSS', 'J0378', 'J0430', 'uSDSS', 'dm_j0660', 'J0660', 'J0410', 'J0515', 'J0861', 'class']\n",
      "All colours: 10\n",
      "['dm_j0660', 'J0378', 'J0395', 'J0410', 'J0430', 'J0515', 'J0660', 'J0861', 'uSDSS', 'gSDSS', 'rSDSS', 'iSDSS', 'zSDSS', 'uSDSS - gSDSS', 'uSDSS - rSDSS', 'uSDSS - iSDSS', 'uSDSS - zSDSS', 'gSDSS - rSDSS', 'gSDSS - iSDSS', 'gSDSS - zSDSS', 'rSDSS - iSDSS', 'rSDSS - zSDSS', 'iSDSS - zSDSS'] 23\n",
      "[0.04010768632720834, 0.0589785911, 0.0621594563, 0.0520535745, 0.0487733074, 0.0379384756, 0.0307152551, 0.0340617932, 0.04083416, 0.009959696, 0.01152701, 0.01310302, 0.02982839, 0.042031228477383525, 0.04242994912141305, 0.042884936236702045, 0.0505683841238545, 0.01523376197636408, 0.016458574589945995, 0.03144723190369092, 0.01745167879203889, 0.03197819271835418, 0.03257947180530249]\n"
     ]
    }
   ],
   "source": [
    "#load databases\n",
    "#DataDir = './data/'\n",
    "DataDir = '.'\n",
    "import pickle\n",
    "dset = pickle.load(open('%s/training_full.data'%DataDir))\n",
    "print dset.keys()\n",
    "\n",
    "\n",
    "featnames = ['dm_j0660','J0378','J0395','J0410','J0430','J0515',\n",
    "             'J0660','J0861','uSDSS','gSDSS','rSDSS','iSDSS','zSDSS']\n",
    "\n",
    "ntrain = len(dset['obj'])\n",
    "feat_arr = []\n",
    "err_arr = []\n",
    "import itertools\n",
    "\n",
    "colfeats = ['uSDSS','gSDSS','rSDSS','iSDSS','zSDSS']\n",
    "\n",
    "terms = colfeats # create all colour combinations\n",
    "nterms = len(terms)\n",
    "ncomb = int(nterms*(nterms-1)/2.)\n",
    "print 'All colours:', ncomb\n",
    "comb = list(itertools.combinations(terms,2))\n",
    "lcomb = list(comb)\n",
    "colournames = ['%s - %s'%(x[0], x[1]) for x in list(comb)]\n",
    "\n",
    "for x in range(ntrain):\n",
    "    fx = []\n",
    "    ex = []\n",
    "    for y in featnames:\n",
    "        fx.append(dset[y][x,0])\n",
    "        ex.append(dset[y][x,1])\n",
    "    for z in range(ncomb):\n",
    "        fx.append(dset[lcomb[z][0]][x,0] - dset[lcomb[z][1]][x,0])\n",
    "        ex.append(np.sqrt((dset[lcomb[z][0]][x,1])**2 + (dset[lcomb[z][1]][x,1])**2))\n",
    "    \n",
    "    feat_arr.append(fx)\n",
    "    err_arr.append(ex)\n",
    "class_arr = dset['class']\n",
    "featnames += colournames\n",
    "print featnames, len(featnames)\n",
    "nfeat = len(featnames)\n",
    "\n",
    "print err_arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a resampled set is generated using `SMOTE`. This will be used as training set. However, protoypes will be drawn from the original training set, which has errors associated. However, I'm not sure of the impact of class imbalance here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling data... done\n",
      "[(0, 666), (1, 666), (2, 666), (3, 666)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "Xt = feat_arr\n",
    "y  = dset['class']\n",
    "\n",
    "# Re-sampling using SMOTE to balance classes\n",
    "X_res, Y_res = SMOTE(sampling_strategy='not majority').fit_resample(Xt, y)\n",
    "\n",
    "def class_to_int(istr):\n",
    "    if istr == 'Halpha':\n",
    "        return 0\n",
    "    elif istr == 'OIII+Hbeta':\n",
    "        return 1\n",
    "    if istr == 'OII':\n",
    "        return 2\n",
    "    elif istr == 'contaminant':\n",
    "        return 3\n",
    "    else:\n",
    "        print '%s not recognised'%istr\n",
    "        return -99\n",
    "\n",
    "\n",
    "y_rsc = [class_to_int(x) for x in Y_res]\n",
    "y_train = [class_to_int(x) for x in y]\n",
    "\n",
    "Scaledata = True\n",
    "if Scaledata:\n",
    "    print 'scaling data...',\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_res)\n",
    "    x_rsc = scaler.transform(X_res)\n",
    "    x_train  = scaler.transform(Xt)\n",
    "    print 'done'\n",
    "\n",
    "outdim = len(np.unique(y_rsc))\n",
    "print(sorted(Counter(y_rsc).items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "joining all results...\n",
      "error:0.999, error_0:99.900, d_err:-98.901, len(objs):31\n",
      "joining all results...\n",
      "error:0.999, error_0:0.999, d_err:-0.001, len(objs):30\n",
      "joining all results...\n",
      "error:0.998, error_0:0.999, d_err:-0.001, len(objs):29\n",
      "joining all results...\n",
      "error:0.998, error_0:0.998, d_err:-0.001, len(objs):28\n",
      "joining all results...\n",
      "error:0.997, error_0:0.998, d_err:-0.001, len(objs):27\n",
      "joining all results...\n",
      "error:0.997, error_0:0.997, d_err:-0.001, len(objs):26\n",
      "joining all results...\n",
      "error:0.996, error_0:0.997, d_err:-0.001, len(objs):25\n",
      "joining all results...\n",
      "error:0.996, error_0:0.996, d_err:-0.001, len(objs):24\n",
      "joining all results...\n",
      "error:0.995, error_0:0.996, d_err:-0.001, len(objs):23\n",
      "joining all results...\n",
      "error:0.995, error_0:0.995, d_err:-0.001, len(objs):22\n",
      "joining all results...\n",
      "error:0.994, error_0:0.995, d_err:-0.001, len(objs):21\n"
     ]
    }
   ],
   "source": [
    "# Choose prototypes\n",
    "import multiprocessing as mp\n",
    "\n",
    "nproc = 4  # number of parallel processes\n",
    "prot_mu = [] # list of ids with all prototypes found\n",
    "prot_be = []\n",
    "temp_prot_id = [] #\n",
    "err_t0 = 100\n",
    "err_t = 99.9\n",
    "nprot= 1\n",
    "\n",
    "def get_pred(z):\n",
    "    pred = [np.round(x) for x in z]\n",
    "    gt3 = pred >3\n",
    "    pred[gt3] = 3 # those that were rounded to 4 are set to 3\n",
    "    return pred\n",
    "\n",
    "\n",
    "beta_arr = 1./(2*np.array(err_arr)**2)\n",
    "idlist = list(np.random.choice(ntrain, 32))\n",
    "max_prot = len(idlist)\n",
    "print max_prot\n",
    "\n",
    "all_errors = []\n",
    "\n",
    "\n",
    "while ((err_t0 >= err_t) and (nprot < max_prot)) or nprot < 5: # while error improves and number of prototypes < max_prot\n",
    "    err_t0 = err_t\n",
    "    errors = []\n",
    "    i0 = 0\n",
    "    i1 = len(idlist)\n",
    "    if i1 > nproc: # if there's a need to parallelise\n",
    "        npp = int((i1-i0+1.0)/(nproc+0.0)) # number of prototype testings per processor\n",
    "\n",
    "        def get_errors(idlist, _i0, _i1):\n",
    "            for _it in range(_i0,_i1):\n",
    "    #    for it in idlist:\n",
    "                it = idlist[_it]\n",
    "                mu_it = [x_train[it]]\n",
    "                beta_it = [beta_arr[it]]\n",
    "                if len(prot_mu) > 0:\n",
    "                    mu_arr = list(prot_mu) ; mu_arr.append(mu_it)\n",
    "                    b_arr  = list(prot_be) ; b_arr.append(beta_it)\n",
    "        #            print 'length of mu_arr %d'%len(mu_arr)\n",
    "                else:\n",
    "                    mu_arr = [mu_it]\n",
    "                    b_arr = [beta_it]\n",
    "\n",
    "#                if it == idlist[0]:\n",
    "#                     print 'prototypes found %d'%len(prot_mu)\n",
    "                rbf = RBF(nfeat,mu_arr,b_arr,outdim)\n",
    "                tr = rbf.train(x_rsc,y_rsc)\n",
    "                pred = get_pred(rbf.test(x_rsc))\n",
    "                errors.append(1 - balanced_accuracy_score(y_rsc, pred,adjusted=True))\n",
    "              #  print 'Error rate: %.4f'% errors[-1]\n",
    "            return errors\n",
    "\n",
    "        def run_prot_test(ip):\n",
    "            _i0 = npp*ip\n",
    "            _i1 = npp*(ip+1)\n",
    "            if ip == nproc-1:\n",
    "                _i1 = i1  # last process is forced to reach the last sub-volume\n",
    "            return get_errors(idlist, _i0, _i1)\n",
    "\n",
    "        pool = mp.Pool(processes=nproc)\n",
    "        res = [pool.apply_async(run_prot_test, args=(x,)) for x in range(nproc)]\n",
    "        results = [p.get() for p in res]\n",
    "        print 'joining all results...'\n",
    "        errors = np.concatenate([results[i] for i in range(nproc)])\n",
    "        pool.terminate()\n",
    "        pool.join()\n",
    "\n",
    "    else:\n",
    "        print 'not running in parallel'\n",
    "        errors = get_errors(idlist, 0, len(idlist))\n",
    "        \n",
    "    best_prot = np.argmin(errors)\n",
    "    prot_mu.append(x_train[idlist[best_prot]])\n",
    "    prot_be.append(beta_arr[idlist[best_prot]])\n",
    "    del idlist[best_prot]\n",
    "    err_t = errors[best_prot]\n",
    "    nprot = len(prot_mu)\n",
    "    all_errors.append(err_t)\n",
    "    print 'error:%.3f, error_0:%.3f, d_err:%.3f, len(objs):%d'%(err_t, err_t0, err_t-err_t0,len(idlist))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
