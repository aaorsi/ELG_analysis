{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jplus\n",
    "import get_3dhst as hst\n",
    "from astropy.io import fits\n",
    "import elgtools as elg\n",
    "import MockJPLUS as mtools\n",
    "import matplotlib.pyplot as plt\n",
    "import Read_Mocks as read\n",
    "\n",
    "\n",
    "# Loading J-PLUS data\n",
    "print 'Loading J-PLUS'\n",
    "mag_type = 'gpsfMags'\n",
    "gal_jplus = jplus.datasets.fetch_jplus_objects(mag_type=mag_type, overwrite=False, \n",
    "                                                 object_name=\"jplus_pz\", nchunks=20, mag_limit=[16,24.],\n",
    "                                                extra_where_conds='',db='dr1', allphotoz=True, upperlimparams=True)\n",
    "\n",
    "\n",
    "#print 'Computing 3FM'\n",
    "#dm_min = 0.0 # rough number, to be computed as a function using the actual trumpet plots for individual tiles.\n",
    "\n",
    "#gal_jplus['dm'] = mtools.gen_3fm(gal_jplus['J0660'][:,0],  gal_jplus['rJAVA'][:,0], gal_jplus['gJAVA'][:,0], \n",
    "#                         Broad_NoLineName='gSDSS')\n",
    "\n",
    "#gal_jplus['err_dm'] = mtools.gen_3fm_err(gal_jplus['J0660'][:,0], gal_jplus['J0660'][:,1], \n",
    "#                                         gal_jplus['rJAVA'][:,0], gal_jplus['rJAVA'][:,1], \n",
    "#                          gal_jplus['gJAVA'][:,0], gal_jplus['gJAVA'][:,1],Broad_NoLineName='gSDSS')\n",
    "\n",
    "print 'setting redshift ranges of ELGs'\n",
    "f_j0660 = jplus.datasets.fetch_jplus_filter('J0660') # Filter transmission curve\n",
    "\n",
    "w_oii = 3727.0 # OII rest-frame\n",
    "z_oii = elg.zline(w_oii, f_j0660.wave, f_j0660.throughput)\n",
    "\n",
    "w_ha = 6563.0\n",
    "z_ha = elg.zline(w_ha, f_j0660.wave, f_j0660.throughput)\n",
    "\n",
    "w_hb = 4861.0\n",
    "z_hb = elg.zline(w_hb, f_j0660.wave, f_j0660.throughput)\n",
    "        \n",
    "w_oiii = 5007.0\n",
    "z_oiii = elg.zline(w_oiii, f_j0660.wave, f_j0660.throughput)\n",
    "\n",
    "print z_oii, z_ha, z_hb, z_oiii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Match with SDSS Phot and add SDSS magnitudes to master, gal_jplus catalogue\n",
    "\n",
    "AddSDSSPhot = True # This adds SDSS magnitudes to xmatches with gal_jplus, but then removes the original gal_jplus and replaces it with the xmatches\n",
    "LoadSDSSxJPLUS = True # if False it will perform the cross match. Otherwise it reads a file\n",
    "\n",
    "xmfile = 'jplus_sdssphot.data'\n",
    "import pickle\n",
    "if AddSDSSPhot:\n",
    "\n",
    "    if LoadSDSSxJPLUS:\n",
    "        print 'loading x-match'\n",
    "        gal_jplus = pickle.load(open(xmfile))\n",
    "        numf = len(gal_jplus['tile_id'])\n",
    "        print 'Number of objects:%ld'%numf\n",
    "    else:\n",
    "        import deepdish as dp\n",
    "        sdssfile = '/home/CEFCA/aaorsi/work/sdss/sdss_phot.h5'\n",
    "        # Due to memory restrictions, the file is loaded by chunks\n",
    "\n",
    "        print 'Loading SDSSPhot coordinates'\n",
    "        sdss_coords = dp.io.load(sdssfile,'/coords')\n",
    "\n",
    "        ngal0 = len(gal_jplus['tile_id'])\n",
    "\n",
    "        print 'Cross-match of SDSSPhot with J-PLUS ...'\n",
    "        d,ind = jplus.tools.crossmatch_angular(gal_jplus['coords'],sdss_coords,max_distance=1e-3)\n",
    "        m = ((d != np.inf))\n",
    "        gal_jplus = jplus.tools.select_object(gal_jplus, m)    \n",
    "        numf = len(gal_jplus['tile_id'])\n",
    "        print 'xmatch catalogue contains %ld galaxies, %.2f of the input J-PLUS catalogue' % (numf, float(numf)/ngal0)\n",
    "\n",
    "        print 'Adding SDSS filters...'\n",
    "\n",
    "        sdssmags = ['u','g','r','i','z']\n",
    "\n",
    "        del sdss_coords\n",
    "        for mag in sdssmags:\n",
    "            magname = '%sSDSS'%mag\n",
    "            print magname\n",
    "            datamag = dp.io.load(sdssfile,'/'+magname)\n",
    "            gal_jplus[magname] = datamag[ind[m]]\n",
    "            del datamag\n",
    "\n",
    "        with open(xmfile,'wb') as outfile:\n",
    "            pickle.dump(gal_jplus,outfile,protocol=pickle.HIGHEST_PROTOCOL)   \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute sigma curves for each tile\n",
    "%matplotlib inline\n",
    "ComputeSigmas = True\n",
    "import pickle\n",
    "\n",
    "if ComputeSigmas:\n",
    "    import matplotlib.gridspec as gsc\n",
    "    reload(elg)\n",
    "    alltiles = np.random.permutation(np.unique(gal_jplus['tile_id']))\n",
    "    ntiles = len(alltiles)\n",
    "    print 'Number of tiles:%d'%ntiles\n",
    "    #sigma_tiles = []\n",
    "\n",
    "    npanels = 10\n",
    "\n",
    "    gs = gsc.GridSpec(npanels,1)\n",
    "    gs.update(wspace=0.0, hspace=0.0, top=4)\n",
    "    gal_jplus['ids'] = np.arange(0,len(gal_jplus['tile_id']))\n",
    "    gal_jplus['elgcand'] = np.zeros(len(gal_jplus['tile_id']))\n",
    "    gal_jplus['dm_j0660'] = np.zeros(len(gal_jplus['tile_id']))\n",
    "    gal_jplus['dm_j0660_err'] = np.zeros(len(gal_jplus['tile_id']))\n",
    "    nc = 0\n",
    "    for i in range(ntiles):\n",
    "    \n",
    "    #for i in range(20):\n",
    "        if i < npanels:\n",
    "            ax = plt.subplot(gs[i])\n",
    "        \n",
    "        idcand,dm, dm_err = elg.continuum_curve(gal_jplus, alltiles[i], BroadLineName = 'rSDSS',\n",
    "                 BroadNoLineName='gSDSS', Plot=ax if i < npanels else False,\n",
    "                 sigma_threshold = 3.0)\n",
    "        #sigma_tiles.append(sfunc)\n",
    "        \n",
    "        gal_jplus['elgcand'][idcand] = 1\n",
    "        gal_jplus['dm_j0660'][idcand] = dm\n",
    "        gal_jplus['dm_j0660_err'][idcand] = dm_err\n",
    "        if i == npanels:\n",
    "            ax.set_xlabel(r'$rSDSS$', fontsize=15)\n",
    "        #plt.title('TileID: %d'%alltiles[i],fontsize=15)\n",
    "        nc += len(idcand)\n",
    "        \n",
    "\n",
    "\n",
    "    elg_cand = jplus.tools.select_object(gal_jplus,gal_jplus['elgcand'] == 1)\n",
    "    print 'saving candidate list...'\n",
    "    \n",
    "    with open('elg_cand.data', 'wb') as outfile:\n",
    "        pickle.dump(elg_cand, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    elg_cand = pickle.load(open('elg_cand.data'))\n",
    "\n",
    "print 'Number of ELG candidates: %d'%len(elg_cand['tile_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(elg_cand['rJAVA'][:,0],elg_cand['rJAVA'][:,0] - elg_cand['rSDSS'][:,0],'k,')\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([16,24])\n",
    "plt.xlabel('rJAVA')\n",
    "plt.ylabel('rJAVA - rSDSS')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print elg_cand.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose which base catalogue to use when constructing the dataset\n",
    "dc = elg_cand\n",
    "suff = 'SDSS'\n",
    "\n",
    "dm = mtools.gen_3fm(dc['J0660'][:,0],  \n",
    "                    dc['r'+suff][:,0], dc['g'+suff][:,0], \n",
    "                    Broad_NoLineName='gSDSS')\n",
    "\n",
    "mask = ((dm > 0.35) & # a minimum EW limit -- not sure what this corresponds to\n",
    "        (dc['r'+suff][:,0] > 20) &  # remove most of H-alpha contaminants\n",
    "        (dc['mask_flags_J0660'] == 0) & # no artifacts is J0660 photometry\n",
    "        (dc['mask_flags_rJAVA'] == 0) &\n",
    "        (dc['single_detect_J0660'] != 0) & # Objects are detected in both rJAVA and J0660\n",
    "        (1./dc['J0660'][:,1] > 5) &    # SNR of J0660 is above 5\n",
    "         (1./dc['r'+suff][:,1] > 5) &  # SNR of SDSS r-band is above 5\n",
    "        (1./dc['rJAVA'][:,1] > 5)     # also SNR of rJAVA is above 5\n",
    "         )\n",
    "\n",
    "dcat = jplus.tools.select_object(dc, mask)\n",
    "\n",
    "print 'Total number of J-PLUS objects for cross-matches: %d'%len(dcat['tile_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dcat['dm_j0660'], bins=20, color='green', alpha=0.5,range=[-2,2])\n",
    "\n",
    "\n",
    "plt.xlabel(r'$C^{r,i} - J0660$',fontsize=20)\n",
    "plt.xlim([-2,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Loading 3D-HST and DEEP2'\n",
    "# Loading 3D-HST and DEEP2 DR4 data\n",
    "\n",
    "hstdata = hst.select_3DHST_z(0,5)#,LineName='Ha_flux',LineMin=0.001)\n",
    "nhst = len(hstdata['ra'])\n",
    "hst_coords = np.zeros([nhst,2])\n",
    "\n",
    "for i in range(nhst):\n",
    "    hst_coords[i,:] = [hstdata['ra'][i],hstdata['dec'][i]]\n",
    "hstdata['coords'] = hst_coords\n",
    "\n",
    "deep2_data = '/home/CEFCA/aaorsi/work/elg_jplus/spec/deep2/dr4/zcat.deep2.dr4.fits'\n",
    "deep2 = fits.open(deep2_data)[1].data\n",
    "\n",
    "\n",
    "# XMatch with 3DHST\n",
    "print 'xmatch with 3DHST'\n",
    "d,ind = jplus.tools.crossmatch_angular(dcat['coords'],hstdata['coords'],max_distance=3e-3)\n",
    "m = ((d != np.inf))\n",
    "\n",
    "jhst0 = jplus.tools.select_object(dcat, m)\n",
    "jhst0['z_spec'] = hstdata['z'][ind[m]]\n",
    "\n",
    "elg_hst =  (\n",
    "            (((jhst0['z_spec'] > z_ha[0])   & (jhst0['z_spec'] < z_ha[1]))   |\n",
    "           ((jhst0['z_spec'] > z_hb[0])   & (jhst0['z_spec'] < z_hb[1]))   |\n",
    "           ((jhst0['z_spec'] > z_oiii[0]) & (jhst0['z_spec'] < z_oiii[1])) |\n",
    "           ((jhst0['z_spec'] > z_oii[0])  & (jhst0['z_spec'] < z_oii[1]))))\n",
    "\n",
    "jhst = jplus.tools.select_object(jhst0, elg_hst)\n",
    "nhst = len(jhst['z_spec'])\n",
    "jhst['idd'] = np.arange(nhst)\n",
    "\n",
    "\n",
    "# xmatch with DEEP2\n",
    "print 'xmatch with DEEP2'\n",
    "d2mask = ((deep2['ZQUALITY'] > 2) &  # Select only Deep2 objects with good quality Redshifts\n",
    "        (deep2['Z'] <= z_ha[1]) |\n",
    "           ((deep2['Z'] >= z_hb[0])   &   (deep2['Z'] <= z_hb[1]))   |\n",
    "           ((deep2['Z'] >= z_oiii[0]) &   (deep2['Z'] <= z_oiii[1])) |\n",
    "           ((deep2['Z'] >= z_oii[0])  &   (deep2['Z'] <= z_oii[1])))\n",
    "          \n",
    "          \n",
    "ndeep2 = len(deep2['RA'][d2mask])\n",
    "dcoords = np.asarray([[deep2['RA'][d2mask][i], deep2['DEC'][d2mask][i]] for i in range(ndeep2)])\n",
    "d,ind = jplus.tools.crossmatch_angular(dcat['coords'],dcoords,max_distance=3e-3)\n",
    "m = ((d != np.inf))\n",
    "\n",
    "jdeep2 = jplus.tools.select_object(dcat, m )\n",
    "jdeep2['z_spec'] = deep2['Z'][d2mask][ind[m]]\n",
    "ndeep2 = len(jdeep2['z_spec'])  \n",
    "#jdeep2['idd'] = np.arange(ndeep2)\n",
    "print jdeep2['z_spec']\n",
    "print jhst['z_spec']\n",
    "print 'Done loading data!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Reading and performing cross-match with ALHAMBRA objects'\n",
    "import deepdish as dd\n",
    "AlhambraFile = '/home/CEFCA/aaorsi/work/alhambra/Alhambra.hdf5'\n",
    "\n",
    "alh = dd.io.load(AlhambraFile)\n",
    "print alh.keys()\n",
    "\n",
    "\n",
    "alh_d,alh_ind = jplus.tools.crossmatch_angular(dcat['coords'],alh['coords'],max_distance=3e-4)\n",
    "alh_m = ((alh_d != np.inf))\n",
    "\n",
    "j_alh = jplus.tools.select_object(dcat, alh_m)\n",
    "print 'Cross match between Alhambra and J-PLUS results in %ld objects' % len(j_alh['tile_id'])\n",
    "j_alh['z_spec'] = alh['zphoto'][alh_ind[alh_m]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 5, 5\n",
    "\n",
    "mask_elgs =  (\n",
    "            (((j_alh['z_spec'] > z_ha[0])   & (j_alh['z_spec'] < z_ha[1]))   |\n",
    "           ((j_alh['z_spec'] > z_hb[0])   & (j_alh['z_spec'] < z_hb[1]))   |\n",
    "           ((j_alh['z_spec'] > z_oiii[0]) & (j_alh['z_spec'] < z_oiii[1])) |\n",
    "           ((j_alh['z_spec'] > z_oii[0])  & (j_alh['z_spec'] < z_oii[1]))))\n",
    "\n",
    "\n",
    "plt.hist(j_alh['z_spec'][mask_elgs], bins=50)\n",
    "nalh = len(j_alh['z_spec'][mask_elgs])\n",
    "\n",
    "j_alhambra = jplus.tools.select_object(j_alh, mask_elgs)\n",
    "\n",
    "print nalh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading SDSS data\n",
    "\n",
    "gal_sdss_spec = jplus.datasets.fetch_sdss_objects(mag_type=\"aperMags\",overwrite=False,mag_limit=[16,21], extinction=False,\n",
    "                                                          clean = True, nchunks = 5, spectroscopic=True, \n",
    "                                                    casjobs=True, object_name=\"galaxies\")\n",
    "\n",
    "print 'Cross-match of SDSS phot with J-PLUS ...'\n",
    "d,ind = jplus.tools.crossmatch_angular(dcat['coords'],gal_sdss_spec['coords'],max_distance=3e-4)\n",
    "m = ((d != np.inf))\n",
    "\n",
    "j_spec = jplus.tools.select_object(dcat, m)\n",
    "\n",
    "        \n",
    "print 'xmatch catalogue contains %ld galaxies' % len(j_spec['tile_id'])\n",
    "        #jplus['in_sdss'] = m\n",
    "        # Adding SDSS magnitudes\n",
    "j_spec['rSDSS'] = gal_sdss_spec['rSDSS'][ind[m]]\n",
    "j_spec['iSDSS'] = gal_sdss_spec['iSDSS'][ind[m]]\n",
    "j_spec['uSDSS'] = gal_sdss_spec['uSDSS'][ind[m]]\n",
    "j_spec['gSDSS'] = gal_sdss_spec['gSDSS'][ind[m]]\n",
    "j_spec['zSDSS'] = gal_sdss_spec['zSDSS'][ind[m]]\n",
    "j_spec['z_spec'] = gal_sdss_spec['zspec'][ind[m]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 5, 5\n",
    "\n",
    "mask_elgs =  (\n",
    "            (((j_spec['z_spec'] > z_ha[0])   & (j_spec['z_spec'] < z_ha[1]))   |\n",
    "           ((j_spec['z_spec'] > z_hb[0])   & (j_spec['z_spec'] < z_hb[1]))   |\n",
    "           ((j_spec['z_spec'] > z_oiii[0]) & (j_spec['z_spec'] < z_oiii[1])) |\n",
    "           ((j_spec['z_spec'] > z_oii[0])  & (j_spec['z_spec'] < z_oii[1]))))\n",
    "\n",
    "\n",
    "plt.hist(j_spec['z_spec'][mask_elgs], bins=50)\n",
    "nsdss = len(j_spec['z_spec'][mask_elgs])\n",
    "\n",
    "j_sdss = jplus.tools.select_object(j_spec, mask_elgs)\n",
    "\n",
    "j_sdss_interlopers = jplus.tools.select_object(j_spec, ~mask_elgs) #interlopers, i.e. outside ELG redshifts\n",
    "\n",
    "nsdss_inter = len(j_sdss_interlopers['z_spec'])\n",
    "\n",
    "\n",
    "print '%d SDSS ELGs, %d interlopers' % (nsdss, nsdss_inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interlopers, stars and QSOs:\n",
    "\n",
    "stars_sdss_spec = jplus.datasets.fetch_sdss_objects(mag_type=\"aperMags\",overwrite=False,mag_limit=[16,21], extinction=False,\n",
    "                                                          clean = True, nchunks = 5, spectroscopic=True, \n",
    "                                                    casjobs=True, object_name=\"stars\")\n",
    "\n",
    "qso_sdss_spec = jplus.datasets.fetch_sdss_objects(mag_type=\"aperMags\",overwrite=False,mag_limit=[16,21], extinction=False,\n",
    "                                                          clean = True, nchunks = 5, spectroscopic=True, \n",
    "                                                    casjobs=True, object_name=\"qso\")\n",
    "\n",
    "\n",
    "print 'Cross-match of SDSS QSOs with ELG cands ...'\n",
    "d,ind = jplus.tools.crossmatch_angular(dcat['coords'],qso_sdss_spec['coords'],max_distance=3e-4)\n",
    "m = ((d != np.inf))\n",
    "\n",
    "q_spec = jplus.tools.select_object(dcat, m)\n",
    "\n",
    "        \n",
    "print 'xmatch catalogue contains %ld galaxies' % len(j_spec['tile_id'])\n",
    "        #jplus['in_sdss'] = m\n",
    "        # Adding SDSS magnitudes\n",
    "q_spec['rSDSS'] = qso_sdss_spec['rSDSS'][ind[m]]\n",
    "q_spec['iSDSS'] = qso_sdss_spec['iSDSS'][ind[m]]\n",
    "q_spec['uSDSS'] = qso_sdss_spec['uSDSS'][ind[m]]\n",
    "q_spec['gSDSS'] = qso_sdss_spec['gSDSS'][ind[m]]\n",
    "q_spec['zSDSS'] = qso_sdss_spec['zSDSS'][ind[m]]\n",
    "q_spec['z_spec'] = qso_sdss_spec['zspec'][ind[m]]\n",
    "\n",
    "nqso = len(q_spec['z_spec'])\n",
    "\n",
    "\n",
    "print 'Cross-match of SDSS stars with ELG cands ...'\n",
    "d,ind = jplus.tools.crossmatch_angular(dcat['coords'],stars_sdss_spec['coords'],max_distance=3e-4)\n",
    "m = ((d != np.inf))\n",
    "\n",
    "s_spec = jplus.tools.select_object(dcat, m)\n",
    "\n",
    "        \n",
    "print 'xmatch catalogue contains %ld galaxies' % len(j_spec['tile_id'])\n",
    "        #jplus['in_sdss'] = m\n",
    "        # Adding SDSS magnitudes\n",
    "s_spec['rSDSS'] = stars_sdss_spec['rSDSS'][ind[m]]\n",
    "s_spec['iSDSS'] = stars_sdss_spec['iSDSS'][ind[m]]\n",
    "s_spec['uSDSS'] = stars_sdss_spec['uSDSS'][ind[m]]\n",
    "s_spec['gSDSS'] = stars_sdss_spec['gSDSS'][ind[m]]\n",
    "s_spec['zSDSS'] = stars_sdss_spec['zSDSS'][ind[m]]\n",
    "s_spec['z_spec'] = np.zeros(len(ind[m])) #stars\n",
    "\n",
    "nstars = len(s_spec['z_spec'])\n",
    "\n",
    "print 'ELG cands include %d stars and %d known QSOs' % (nstars, nqso)\n",
    "\n",
    "\n",
    "ncontaminants = nstars + nqso + nsdss_inter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Lightcone galaxies\n",
    "nCone = 512\n",
    "lcone = []\n",
    "\n",
    "Mockpath = '/home/CEFCA/aaorsi/work/JPLUS_Mock/new/Lines/'\n",
    "namelc   = 'LightCone_SA_0_'\n",
    "\n",
    "print 'reading Lightcone...'\n",
    "for i in range(nCone): #nCone):\n",
    "    nameIn = '%s%s%d' % (Mockpath, namelc, i)\n",
    "    ilc = read.readmock_chunk_PythonCut(nameIn, zspace = True)\n",
    "    lcone.append(ilc[0])\n",
    "    \n",
    "dcone = np.concatenate(lcone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import learn_elgs as learn\n",
    "tfout = '/home/CEFCA/aaorsi/work/elg_jplus/trainspec.dat'\n",
    "allspec, photo_spec = learn.Load_Synthetic_Sample(tfout,overwrite=False, sdssxjplus=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(allspec)\n",
    "print len(dcone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.gridspec as gsc\n",
    "\n",
    "training_frac = 0.7 # the rest is for testing\n",
    "ngals_tot = ndeep2 + nhst + nsdss + nalh + ncontaminants\n",
    "print 'Total number of objects in data set: %d'%ngals_tot\n",
    "\n",
    "dataspec = {}\n",
    "\n",
    "for key in jdeep2:\n",
    "    if key == 'date' or key == 'SQL_query' or key == 'filename':\n",
    "        continue\n",
    "    counter = 0\n",
    "    value_hst   = jhst[key]\n",
    "    value_deep2 = jdeep2[key]\n",
    "    value_sdss  = j_sdss[key]\n",
    "    value_alh   = j_alhambra[key]\n",
    "    value_int   = j_sdss_interlopers[key]\n",
    "    value_star  = s_spec[key]\n",
    "    value_qso   = q_spec[key]\n",
    "    shape = value_deep2.shape\n",
    "    ndim = value_deep2.ndim\n",
    "    dataspec[key] = np.zeros(ngals_tot) if ndim == 1 else np.zeros([ngals_tot, shape[1]])\n",
    "    counter += ndeep2\n",
    "    dataspec[key][0:counter] = value_deep2\n",
    "    dataspec[key][counter:counter+nhst] = value_hst   \n",
    "    counter += nhst\n",
    "    dataspec[key][counter:counter+nsdss] = value_sdss   \n",
    "    counter += nsdss\n",
    "    dataspec[key][counter:counter+nalh] = value_alh   \n",
    "    counter += nalh\n",
    "    dataspec[key][counter:counter+nstars] = value_star   \n",
    "    counter += nstars\n",
    "    dataspec[key][counter:counter+nqso] = value_qso   \n",
    "    counter += nqso\n",
    "    dataspec[key][counter:counter+nsdss_inter] = value_int   \n",
    "    counter += nsdss_inter\n",
    "    \n",
    "    \n",
    "\n",
    "dataspec['type'] = []\n",
    "for i in range(ndeep2):\n",
    "    dataspec['type'].append('DEEP2')\n",
    "for i in range(nhst):\n",
    "    dataspec['type'].append('3D-HST')\n",
    "for i in range(nsdss):\n",
    "    dataspec['type'].append('SDSS')\n",
    "for i in range(nalh):\n",
    "    dataspec['type'].append('Alhambra')\n",
    "for i in range(ncontaminants):\n",
    "    dataspec['type'].append('SDSS contaminants')\n",
    "    \n",
    "dataspec['index'] = np.arange(ndeep2 + nhst+nsdss+nalh)    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 14**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all dataset\n",
    "Plot_TrainingSet = True\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if Plot_TrainingSet:\n",
    "\n",
    "    filternames = [ 'J0378','J0395', 'J0410','J0430','J0515', 'J0660',\n",
    "                   'J0861','uJAVA', 'gJAVA', 'rJAVA', 'iJAVA','zJAVA']\n",
    "                #  'uSDSS', 'gSDSS', 'rSDSS', 'iSDSS', 'zSDSS']\n",
    "\n",
    "    sdss_fnames = ['uSDSS', 'gSDSS', 'rSDSS', 'iSDSS', 'zSDSS']\n",
    "    sdss_mw     = [3543, 4770, 6231, 7625, 9134]\n",
    "\n",
    "  \n",
    "    grid = [14,14]\n",
    "    gs = gsc.GridSpec(grid[0],grid[1])\n",
    "    gs.update(wspace=0.035, hspace=0.035)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = 60, 60\n",
    "\n",
    "    \n",
    "    nxm = len(dataspec['tile_id'])\n",
    "    print 'Number of xmatched ELGs: %ld'%nxm\n",
    "    dataspec['dm'] = mtools.gen_3fm(dataspec['J0660'][:,0], dataspec['rJAVA'][:,0], \n",
    "                              dataspec['iJAVA'][:,0],Broad_NoLineName='iSDSS')\n",
    "\n",
    "    ix = 0\n",
    "    iy = 0\n",
    "    idg = 0\n",
    "\n",
    "    sortz = np.argsort(dataspec['z_spec'])\n",
    "\n",
    "    for i0 in range(nxm):\n",
    "        i = sortz[i0]\n",
    "        ax= plt.subplot(gs[ix,iy])\n",
    "        idg = ix + grid[0]*iy\n",
    "        for fname in filternames:\n",
    "            filt = jplus.tools.fetch_jplus_filter(fname, quiet=True)\n",
    "    #        print np.median(filt.wave)\n",
    "            ww = [filt.avgwave(), filt.avgwave()]\n",
    "            ff = [dataspec[fname][i,0], dataspec[fname][i,0]]\n",
    "\n",
    "            ax.plot(ww,ff,'o', color='blue', markersize=10)\n",
    "            ax.errorbar(ww,ff,yerr= dataspec[fname][i,1], color='blue')\n",
    "\n",
    "\n",
    "            if fname == 'J0660':# or fname == 'J0378' or fname == 'J0395' or fname=='J0861':\n",
    "                fwidth = filt.rectwidth()\n",
    "                ax.fill_between([ww[0] - fwidth/2.,ww[0] + fwidth/2.,ww[0] + fwidth/2.,ww[0] - fwidth/2.],\n",
    "                            [17,17, 28, 28],facecolor='Grey',alpha=0.1)\n",
    "        fw = 0   \n",
    "        if sdss_fnames[0] in dataspec: # if sdss bands exist\n",
    "            for fname in sdss_fnames:\n",
    "                filt = jplus.tools.fetch_sdss_filter(fname, quiet=True)\n",
    "    #        ww = [filt[0,filt[1,:].argmax()],filt[0,filt[1,:].argmax()]]\n",
    "                ww = [sdss_mw[fw], sdss_mw[fw]]\n",
    "                fw += 1\n",
    "                ff = [dataspec[fname][i,0], dataspec[fname][i,0]]\n",
    "                ax.plot(ww,ff,'o', color='red', markersize=10)\n",
    "                ax.errorbar(ww,ff,yerr= dataspec[fname][i,1], color='red')\n",
    "\n",
    "        ax.text(0.3,0.9,r'$z_{\\rm spec}=%.2f$'%dataspec['z_spec'][i], transform= ax.transAxes, fontsize=12)\n",
    "        ax.text(0.3,0.8,r'$\\Delta m=%.2f$'%dataspec['dm_j0660'][i], transform= ax.transAxes, fontsize=12, color='red')\n",
    "        ax.text(0.3,0.7,dataspec['type'][i], transform= ax.transAxes, fontsize=12, color='black')\n",
    "        ax.text(0.1,0.9,'(%d)'%idg, transform=ax.transAxes,fontsize=15)\n",
    "        ax.set_ylim([25, 17.99])\n",
    "        ax.set_xlim([3000, 8999])\n",
    "        if ix != grid[0]-1:\n",
    "            ax.set_xticklabels([])\n",
    "        if iy >0:\n",
    "            ax.set_yticklabels([])\n",
    "        ix +=1\n",
    "        if ix == grid[0]:\n",
    "            ix = 0\n",
    "            iy += 1\n",
    "\n",
    "    plt.show()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 5, 5\n",
    "plt.hist(dataspec['z_spec'],bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying the full set of galaxies\n",
    "dataspec['class'] = []\n",
    "for i in range(ngals_tot):\n",
    "    zp = dataspec['z_spec'][i]\n",
    "    if zp < z_ha[1]:\n",
    "        dataspec['class'].append('Halpha')\n",
    "#    elif (zp > z_ha[1]) and (zp < z_oiii[1]):\n",
    "#        dataspec['class'].append('OIII')\n",
    "#    elif (zp > z_oiii[1]) and (zp < z_hb[1]):\n",
    "#        dataspec['class'].append('Hbeta')\n",
    "    elif (zp > z_ha[1]) and (zp < z_hb[1]):\n",
    "        dataspec['class'].append('OIII+Hbeta')\n",
    "    elif (zp > 0.5):\n",
    "        dataspec['class'].append('OII')\n",
    "    else:\n",
    "        print 'something is wrong with this redshift: %f'%zp\n",
    "\n",
    "m_z0  = np.asarray(dataspec['class']) == 'Halpha'\n",
    "m_zp3 = np.asarray(dataspec['class']) == 'OIII+Hbeta'\n",
    "#m_zp35 = np.asarray(dataspec['class']) == 'Hbeta'\n",
    "m_zp7 = np.asarray(dataspec['class']) == 'OII'\n",
    "\n",
    "nz0  = len(dataspec['z_spec'][m_z0])\n",
    "nzp3 = len(dataspec['z_spec'][m_zp3])  \n",
    "#nzp35 = len(dataspec['z_spec'][m_zp35])  \n",
    "nzp7 = len(dataspec['z_spec'][m_zp7])\n",
    "print 'Number of Halpha emitters: %ld' % nz0\n",
    "print 'Number of OIII+Hbeta emitters: %ld' % nzp3       \n",
    "#print 'Number of Hbeta emitters: %ld' % nzp35\n",
    "print 'Number of OII emitters: %ld' % nzp7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot (some) dataset objects\n",
    "\n",
    "Plot_TrainingSet = True\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if Plot_TrainingSet:\n",
    "\n",
    "    filternames = [ 'J0378','J0395', 'J0410','J0430','J0515', 'J0660',\n",
    "                   'J0861','uJAVA', 'gJAVA', 'rJAVA', 'iJAVA','zJAVA']\n",
    "                #  'uSDSS', 'gSDSS', 'rSDSS', 'iSDSS', 'zSDSS']\n",
    "\n",
    "    sdss_fnames = ['uSDSS', 'gSDSS', 'rSDSS', 'iSDSS', 'zSDSS']\n",
    "    sdss_mw     = [3543, 4770, 6231, 7625, 9134]\n",
    "\n",
    "  \n",
    "    grid = [3,4]\n",
    "    gs = gsc.GridSpec(grid[0],grid[1])\n",
    "    gs.update(wspace=0.035, hspace=0.035)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "    \n",
    "    nxm = grid[0]*grid[1]\n",
    "    print 'Number of xmatched ELGs: %ld'%nxm\n",
    "    \n",
    "    ix = 0\n",
    "    iy = 0\n",
    "    idg = 0\n",
    "    \n",
    "    types = np.unique(dataspec['class'])\n",
    "    sortz = []\n",
    "    \n",
    "    for ii in types:\n",
    "        ichoice = np.random.choice(np.where(np.asarray(dataspec['class']) == ii)[0],grid[1])\n",
    "        print ichoice\n",
    "        for tt in range(grid[1]):\n",
    "            sortz.append(ichoice[tt])\n",
    "        \n",
    "    for i0 in range(nxm):\n",
    "        i = sortz[i0]\n",
    "        ax= plt.subplot(gs[ix,iy])\n",
    "        idg = ix + grid[0]*iy\n",
    "        for fname in filternames:\n",
    "            filt = jplus.tools.fetch_jplus_filter(fname, quiet=True)\n",
    "    #        print np.median(filt.wave)\n",
    "            ww = [filt.avgwave(), filt.avgwave()]\n",
    "            ff = [dataspec[fname][i,0], dataspec[fname][i,0]]\n",
    "\n",
    "            ax.plot(ww,ff,'o', color='blue', markersize=10)\n",
    "            ax.errorbar(ww,ff,yerr= dataspec[fname][i,1], color='blue')\n",
    "\n",
    "            if fname == 'J0660':# or fname == 'J0378' or fname == 'J0395' or fname=='J0861':\n",
    "                fwidth = filt.rectwidth()\n",
    "                ax.fill_between([ww[0] - fwidth/2.,ww[0] + fwidth/2.,ww[0] + fwidth/2.,ww[0] - fwidth/2.],\n",
    "                            [17,17, 28, 28],facecolor='Grey',alpha=0.2)\n",
    "        fw = 0   \n",
    "        if sdss_fnames[0] in dataspec: # if sdss bands exist\n",
    "            for fname in sdss_fnames:\n",
    "                filt = jplus.tools.fetch_sdss_filter(fname, quiet=True)\n",
    "    #        ww = [filt[0,filt[1,:].argmax()],filt[0,filt[1,:].argmax()]]\n",
    "                ww = [sdss_mw[fw], sdss_mw[fw]]\n",
    "                fw += 1\n",
    "                ff = [dataspec[fname][i,0], dataspec[fname][i,0]]\n",
    "                ax.plot(ww,ff,'o', color='red', markersize=10)\n",
    "                ax.errorbar(ww,ff,yerr= dataspec[fname][i,1], color='red')\n",
    "\n",
    "        ax.text(0.3,0.9,r'$z_{\\rm spec}=%.2f$'%dataspec['z_spec'][i], transform= ax.transAxes, fontsize=12)\n",
    "        ax.text(0.3,0.8,r'$\\Delta m=%.2f$'%dataspec['dm_j0660'][i], transform= ax.transAxes, fontsize=12, color='red')\n",
    "        ax.text(0.3,0.7,dataspec['type'][i], transform= ax.transAxes, fontsize=12, color='black')\n",
    "        ax.text(0.1,0.9,'(%d)'%idg, transform=ax.transAxes,fontsize=15)\n",
    "        ax.set_ylim([25, 17.99])\n",
    "        ax.set_xlim([3000, 8999])\n",
    "        if ix != grid[0]-1:\n",
    "            ax.set_xticklabels([])\n",
    "        if iy >0:\n",
    "            ax.set_yticklabels([])\n",
    "        iy +=1\n",
    "        if iy == grid[1]:\n",
    "            iy = 0\n",
    "            ix += 1\n",
    "\n",
    "    plt.show()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of some features for all classes\n",
    "\n",
    "snr = 1./dataspec['rSDSS'][:,1]\n",
    "snr_ha = 1./dataspec['J0660'][:,1]\n",
    "\n",
    "#classes_names = ['Halpha', 'OIII', 'Hbeta','OII']\n",
    "classes_names = ['Halpha', 'OIII+Hbeta','OII']\n",
    "colors = plt.cm.coolwarm(np.linspace(0,1,len(classes_names)))\n",
    "iic = 0\n",
    "plt.figure(1,figsize=(10,3))\n",
    "\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(snr[mask], bins=20, color=colors[iic], alpha=0.5, label=ic, range=[2,200])\n",
    "    iic += 1\n",
    "    \n",
    "plt.xlabel('r SNR',fontsize=20)\n",
    "plt.xlim([0,200])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(2,figsize=(10,4))\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(snr_ha[mask], bins=20, color=colors[iic], alpha=0.5, label=ic, range=[2,100])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel('J0660 SNR',fontsize=20)\n",
    "plt.xlim([0,100])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "\n",
    "plt.figure(3,figsize=(10,4))\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['mu_max_r'][mask]-dataspec['rJAVA'][mask,0], bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$\\mu_{\\rm max}- rJAVA$',fontsize=20)\n",
    "#plt.xlim([0,100])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "#plt.figure(4,figsize=(10,4))\n",
    "#suff = 'SDSS'\n",
    "\n",
    "#iic= 0\n",
    "#for ic in classes_names:\n",
    "#    mask = np.asarray(dataspec['class']) == ic\n",
    "#    plt.hist(dataspec['dm_j0660'][mask], bins=20, color=colors[iic], alpha=0.5, label=ic,range=[0,2])\n",
    "#    iic += 1\n",
    "\n",
    "#plt.xlabel(r'$C^{r,g} - J0660$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "#plt.legend()\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(5,figsize=(10,3))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['g'+suff][mask,0] - dataspec['r'+suff][mask,0], bins=20, color=colors[iic], alpha=0.5, label=ic)#,range=[0,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$g-r$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(6,figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['u'+suff][mask,0] - dataspec['z'+suff][mask,0], bins=20, color=colors[iic], alpha=0.5, label=ic)#,range=[0,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$u-z$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(7,figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0861'][mask,0] - dataspec['z'+suff][mask,0], bins=20, color=colors[iic], alpha=0.5, label=ic,range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0861-z$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure(8,figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    right_col = dataspec['r'+suff][mask,0]-dataspec['J0660'][mask,0]\n",
    "    left_col = dataspec['g'+suff][mask,0]-dataspec['J0660'][mask,0]\n",
    "    plt.hist(right_col/left_col, bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$(r-J0660)/(g-J0660)$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0395-g',figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0395'][mask,0] - dataspec['g'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0395-g$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0410-g',figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0410'][mask,0] - dataspec['g'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0410-g$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0430-g',figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0430'][mask,0] - dataspec['g'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0430-g$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0515-g',figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0515'][mask,0] - dataspec['g'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0515-g$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0378-u',figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0378'][mask,0] - dataspec['u'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0378-u$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('j0395-u',figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['J0395'][mask,0] - dataspec['u'+suff][mask,0], bins=10, color=colors[iic], alpha=0.5, \n",
    "             label=ic, range=[-2,2])\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$J0395-u$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "\n",
    "\n",
    "bb = ['u','g','r','i','z']\n",
    "\n",
    "for band in bb:\n",
    "    plt.figure(band,figsize=(10,4))\n",
    "    suff = 'SDSS'\n",
    "\n",
    "    iic= 0\n",
    "    for ic in classes_names:\n",
    "        mask = np.asarray(dataspec['class']) == ic\n",
    "        plt.hist(dataspec[band+suff][mask,0] , bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "        iic += 1\n",
    "\n",
    "    plt.xlabel(r'$%s%s$'%(band,suff),fontsize=20)\n",
    "    #plt.xlim([-0.5,2])\n",
    "    plt.legend()\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "\n",
    "\n",
    "\n",
    "pzarr = ['leph', 'tpz', 'bpz']    \n",
    "\n",
    "for pz in pzarr:\n",
    "    plt.figure(pz,figsize=(10,4))\n",
    "    suff = 'SDSS'\n",
    "\n",
    "    iic= 0\n",
    "    for ic in classes_names:\n",
    "        mask = np.asarray(dataspec['class']) == ic\n",
    "        plt.hist(dataspec['photoz_'+pz][mask], bins=20, color=colors[iic], alpha=0.5, label=ic, range=[0,1])\n",
    "        iic += 1\n",
    "\n",
    "    plt.xlabel(r'$z_{\\rm %s}$'%pz,fontsize=20)\n",
    "    #plt.xlim([-0.5,2])\n",
    "    plt.legend()\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "\n",
    "\n",
    "    \n",
    "plt.figure('dm_j0660',figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['dm_j0660'][mask], bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$C - J0660$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.figure('dm_j0660_err',figsize=(10,4))\n",
    "suff = 'SDSS'\n",
    "\n",
    "iic= 0\n",
    "for ic in classes_names:\n",
    "    mask = np.asarray(dataspec['class']) == ic\n",
    "    plt.hist(dataspec['dm_j0660_err'][mask], bins=20, color=colors[iic], alpha=0.5, label=ic)\n",
    "    iic += 1\n",
    "\n",
    "plt.xlabel(r'$err(C - J0660)$',fontsize=20)\n",
    "#plt.xlim([-0.5,2])\n",
    "plt.legend()\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print dataspec['mu_max_r'][0]\n",
    "print dataspec.keys()\n",
    "#dataspec['cstar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function retrieves a features list for a single object.\n",
    "\n",
    "def prepare_sample(data, index, sample_error = True, sample_type = 'All_Mags', filters='J-PLUS'):\n",
    "    \n",
    "    BBNames = ['u','g','r','i','z']\n",
    "    \n",
    "    suff = 'JAVA' if filters == 'J-PLUS' else 'SDSS'\n",
    "    \n",
    "    filternames = [ 'J0378','J0395', 'J0410','J0430','J0515', 'J0660',\n",
    "                   'J0861','u'+suff, 'g'+suff, 'r'+suff, 'i'+suff,'z'+suff]\n",
    "        \n",
    "        \n",
    "    \n",
    "    flist = [] # filterslist\n",
    "    ferrlist = []\n",
    "    nfilters = len(filternames)\n",
    "    amplitude = np.random.normal(0, 2.0) # amplitude scaling change \n",
    "    for ff in filternames:\n",
    "        if sample_error:\n",
    "            flist.append(np.random.normal(data[ff][index,0], data[ff][index,1]) + amplitude)  # errors distribute like a gaussian (?)\n",
    "        else:\n",
    "            flist.append(data[ff][index,0])\n",
    "        ferrlist.append(data[ff][index,1])\n",
    "    \n",
    "    sample = [] # The sample's features\n",
    "    \n",
    "#    dm_J0515 = mtools.gen_3fm(data['J0515'][index,0], data['g'+suff][index,0], \n",
    "#                          data['r'+suff][index,0],Broad_NoLineName='rSDSS', LineFilterName='J0515', \n",
    "#                          Broad_LineName='gSDSS')\n",
    "#    err_dm_J0515 = mtools.gen_3fm_err(data['J0515'][index,0], data['J0515'][index,1], data['g'+suff][index,0], \n",
    "#                                      data['g'+suff][index,1], data['r'+suff][index,0], data['r'+suff][index,1],\n",
    "#                                      Broad_NoLineName='rSDSS')\n",
    "\n",
    "\n",
    "#    dm_J0378 = mtools.gen_3fm(data['J0378'][index,0], data['u'+suff][index,0], \n",
    "#                          data['g'+suff][index,0],Broad_NoLineName='gSDSS', LineFilterName='J0378', \n",
    "#                              Broad_LineName='uJAVA')\n",
    "##    err_dm_J0378 = mtools.gen_3fm_err(data['J0378'][index,0], data['J0378'][index,1], data['u'+suff][index,0], \n",
    "#                                      data['u'+suff][index,1],data['g'+suff][index,0], data['g'+suff][index,1],\n",
    "#                                      Broad_NoLineName='gSDSS')\n",
    "\n",
    "#    dm_J0861 = mtools.gen_3fm(data['J0861'][index,0], data['z'+suff][index,0], \n",
    "#                          data['i'+suff][index,0],Broad_NoLineName='iSDSS', \n",
    "#                          LineFilterName='J0861', Broad_LineName='zSDSS')\n",
    "\n",
    "#    err_dm_J0861 = mtools.gen_3fm_err(data['J0861'][index,0], data['J0861'][index,1], data['z'+suff][index,0], \n",
    "#                          data['z'+suff][index,1],data['i'+suff][index,0], data['i'+suff][index,1],\n",
    "#                          Broad_NoLineName='iSDSS')\n",
    "\n",
    "    dm_J0660 = mtools.gen_3fm(data['J0660'][index,0], data['r'+suff][index,0], \n",
    "                          data['g'+suff][index,0],Broad_NoLineName='gSDSS', \n",
    "                          LineFilterName='J0660', Broad_LineName='rSDSS')\n",
    "\n",
    "    err_dm_J0660 = mtools.gen_3fm_err(data['J0660'][index,0], data['J0660'][index,1], data['r'+suff][index,0], \n",
    "                          data['r'+suff][index,1],data['g'+suff][index,0], data['g'+suff][index,1],\n",
    "                          Broad_NoLineName='gSDSS')\n",
    "\n",
    "    \n",
    "    # Here I should get creative\n",
    "    if sample_type == 'All_Mags':\n",
    "        sample = flist # All individual filters\n",
    "    if sample_type == 'All_Mags_mumax':\n",
    "        for i in range(nfilters):\n",
    "            sample.append(flist[i])\n",
    "        sample.append(data['mu_max_r'][index] - data['rJAVA'][index,0])\n",
    "        #sample.append(data['cstar'][index])\n",
    "#        sample.append(data['pz_bpz'][index])\n",
    "    if sample_type == 'Colors':\n",
    "        for i in range(nfilters):\n",
    "            sample.append(flist[i])\n",
    "            for j in range(nfilters):\n",
    "                if i != j:\n",
    "                    sample.append(flist[i] - flist[j])\n",
    "                   \n",
    "    if sample_type == 'All':\n",
    "        for i in range(nfilters):\n",
    "            sample.append(flist[i])\n",
    "        sample.append(data['mu_max_r'][index])\n",
    "        #sample.append(data['cstar'][index])\n",
    "        sample.append(data['pz_bpz'][index])\n",
    "        for i in range(nfilters):\n",
    "            sample.append(flist[i])\n",
    "            for j in range(nfilters):\n",
    "                if i != j:\n",
    "                    sample.append(flist[i] - flist[j]) \n",
    "    if sample_type == 'dm':\n",
    "        # Delta-m and SNR of deltaMs...\n",
    "        sample=[#dm_J0515, #1./err_dm_J0515,\n",
    "                #dm_J0378, #1./err_dm_J0378,\n",
    "                #dm_J0861#, 1./err_dm_J0861\n",
    "               dm_J0660, err_dm_J0660 ]\n",
    "                 #data['dm'][index],data['err_dm'][index]\n",
    "                 #]\n",
    "        sample.append(data['mu_max_r'][index] - data['rJAVA'][index,0])\n",
    "        \n",
    "        #for ff in BBNames:\n",
    "        #    sample.append(np.random.normal(data[ff+suff][index,0], data[ff+suff][index,1])\n",
    "        #    if sample_error else data[ff+suff][index,0])\n",
    "        \n",
    "        #u-g\n",
    "        #sample.append(np.random.normal(data['u'+suff][index,0], data['u'+suff][index,1])-\n",
    "        #              np.random.normal(data['g'+suff][index,0], data['g'+suff][index,1])\n",
    "        #              if sample_error else data['u'+suff][index,0]-data['g'+suff][index,0])\n",
    "        #g-r\n",
    "        sample.append(np.random.normal(data['g'+suff][index,0], data['g'+suff][index,1])-\n",
    "                      np.random.normal(data['r'+suff][index,0], data['r'+suff][index,1])\n",
    "                      if sample_error else data['g'+suff][index,0]-data['r'+suff][index,0])\n",
    "\n",
    "        #r-i\n",
    "        sample.append(np.random.normal(data['r'+suff][index,0], data['r'+suff][index,1])-\n",
    "                      np.random.normal(data['i'+suff][index,0], data['i'+suff][index,1])\n",
    "                      if sample_error else data['r'+suff][index,0]-data['i'+suff][index,0])\n",
    "\n",
    "        #i-z\n",
    "        sample.append(np.random.normal(data['i'+suff][index,0], data['i'+suff][index,1])-\n",
    "                      np.random.normal(data['z'+suff][index,0], data['z'+suff][index,1])\n",
    "                      if sample_error else data['i'+suff][index,0]-data['z'+suff][index,0])\n",
    "\n",
    "        #u-z\n",
    "        #sample.append(np.random.normal(data['u'+suff][index,0], data['u'+suff][index,1])-\n",
    "        #             np.random.normal(data['z'+suff][index,0], data['z'+suff][index,1])\n",
    "        #              if sample_error else data['u'+suff][index,0]-data['z'+suff][index,0])\n",
    "        \n",
    "        #r-z\n",
    "        sample.append(np.random.normal(data['r'+suff][index,0], data['r'+suff][index,1])-\n",
    "                     np.random.normal(data['z'+suff][index,0], data['z'+suff][index,1])\n",
    "                      if sample_error else data['r'+suff][index,0]-data['z'+suff][index,0])\n",
    "        \n",
    "        \n",
    "        # colors including NBs\n",
    "        \n",
    "        #\n",
    "        \n",
    "        \n",
    "        \n",
    "        #SNR in r-band\n",
    "        sample.append(1./data['r'+suff][index,1])\n",
    "        #SNR in J0660 band\n",
    "        #sample.append(1./data['J0660'][index,1])\n",
    "        \n",
    "        # r-band\n",
    "        sample.append(np.random.normal(data['r'+suff][index,0] if sample_error else data['r'+suff][index,0]))\n",
    "        #g-band\n",
    "        #sample.append(np.random.normal(data['g'+suff][index,0] if sample_error else data['g'+suff][index,0]))\n",
    "        \n",
    "        \n",
    "        \n",
    "    return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compute_Dataset = True   # if False it reads a dataset from a file\n",
    "\n",
    "Training_set_class_frac = .8 # a fraction X of the less abundant determines the number of objects per class in the training set.\n",
    "Sampling_errors         = 1  # Each training galaxy is resampled X times using its errors\n",
    "\n",
    "# Set only one of these to True\n",
    "Sample_Small_Only = False       # If True, it will resample only the class with the smallest sample\n",
    "Sample_Balancing  = True        # All classes are resampled but the end result is balanced\n",
    "UseFixedNum       = False       # Use a fixed number of objects per class\n",
    "\n",
    "# indices of different categories of objects\n",
    "id_z0   = dataspec['index'][m_z0] \n",
    "id_z0p3 = dataspec['index'][m_zp3]\n",
    "#id_z0p35 = dataspec['index'][m_zp35]\n",
    "id_z0p7 = dataspec['index'][m_zp7]\n",
    "\n",
    "#nzarr = [nz0, nzp3, nzp35, nzp7]\n",
    "nzarr = [nz0, nzp3, nzp7]\n",
    "min_class = np.min(nzarr)\n",
    "id_min    = np.argmin(nzarr)\n",
    "id_max    = np.argmax(nzarr)\n",
    "\n",
    "MaxClass = Sampling_errors * nzarr[id_max] \n",
    "\n",
    "if Sample_Balancing:\n",
    "    print 'MaxClass = %d'%MaxClass\n",
    "\n",
    "balance_factors= [int((MaxClass+0.)/i) for i in nzarr]\n",
    "\n",
    "if UseFixedNum:\n",
    "    numperclass = int(min_class*Training_set_class_frac)\n",
    "    print 'number of objects in training set per class before sampling errors: %d'%numperclass\n",
    "    print 'number of objects left for validation: %d'%(ngals_tot-numperclass*3)\n",
    "    \n",
    "# Randomised lists:\n",
    "ran_idz0 = np.random.permutation(id_z0)\n",
    "ran_idz0p3 = np.random.permutation(id_z0p3)    \n",
    "#ran_idz0p35 = np.random.permutation(id_z0p35)    \n",
    "ran_idz0p7 = np.random.permutation(id_z0p7)\n",
    "\n",
    "#id_arr = [ran_idz0, ran_idz0p3, ran_idz0p35, ran_idz0p7]\n",
    "id_arr = [ran_idz0, ran_idz0p3, ran_idz0p7]\n",
    "\n",
    "training_features = []\n",
    "training_class    = []\n",
    "\n",
    "sample_type='dm'\n",
    "filterset = 'SDSS'\n",
    "\n",
    "if Compute_Dataset:\n",
    "\n",
    "    DoSampling = True if Sampling_errors >= 1 else False\n",
    "\n",
    "    iid = 0\n",
    "    for id_obj in id_arr:\n",
    "        nid = numperclass if UseFixedNum else int(len(id_obj)*Training_set_class_frac)\n",
    "\n",
    "        if Sample_Small_Only:\n",
    "            DoSampling = True if (Sampling_errors >= 1) and (iid == id_min) else False\n",
    "            Serr = 1\n",
    "        else:\n",
    "            Serr = Sampling_errors\n",
    "\n",
    "        if Sample_Balancing:\n",
    "            DoSampling = True\n",
    "            Serr = balance_factors[iid]\n",
    "            print 'Balancing sample %d with resampling %d'% (iid, Serr)\n",
    "        iclass = 0\n",
    "        for i in range(nid):\n",
    "            for j in range(Serr):\n",
    "                training_features.append(prepare_sample(dataspec, id_obj[i], sample_error=DoSampling, \n",
    "                                                        sample_type=sample_type, filters=filterset))\n",
    "                training_class.append(dataspec['class'][id_obj[i]])\n",
    "                iclass += 1\n",
    "        iid += 1\n",
    "        print 'Number of elements in class %d: %d'%(iid, iclass)\n",
    "\n",
    "\n",
    "    if Training_set_class_frac < 1:    # if == 1 then CV is used instead\n",
    "        validate_features = []\n",
    "        validate_class    = []\n",
    "\n",
    "        for id_obj in id_arr:\n",
    "            n0id = numperclass if UseFixedNum else int(len(id_obj)*Training_set_class_frac)\n",
    "            nid = len(id_obj)\n",
    "\n",
    "            for i in range(n0id, nid):\n",
    "                validate_features.append(prepare_sample(dataspec, id_obj[i], sample_error=False, \n",
    "                                                        sample_type=sample_type, filters=filterset))\n",
    "                validate_class.append(dataspec['class'][id_obj[i]])\n",
    "\n",
    "\n",
    "    # Save dataset to a file\n",
    "    dataset = {'tfeatures':training_features,\n",
    "                   'tclass'   :training_class,\n",
    "                   'vfeatures':validate_features,\n",
    "                   'vclass'   :validate_class}\n",
    "\n",
    "    with open('dataset.data','wb') as outfile:\n",
    "        pickle.dump(dataset,outfile,protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "\n",
    "else:\n",
    "    dataset = pickle.load(open('dataset.data'))\n",
    "    training_features = dataset['tfeatures']\n",
    "    training_class    = dataset['tclass']\n",
    "    validate_features = dataset['vfeatures']\n",
    "    validate_class    = dataset['vclass']\n",
    "    \n",
    "\n",
    "print 'number of objects in training set: %ld'%len(training_features)        \n",
    "#for i in range(len(training_features)):\n",
    "#    print training_features[i], training_class[i]\n",
    "    \n",
    "print 'Number of features: %ld' % len(training_features[0])\n",
    "ntrain = len(training_features)     \n",
    "nfeat = len(training_features[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#mlcolors[index]\n",
    "def summary_performance_plot(index,precision, recall, fscore, color, cnum, ctype, name,  gs, figname='summary', ylim = [.0, 1.1]):\n",
    "    \n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax.plot(cnum, precision,'o-', label=name, color=color)\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks([0,1,2,3])\n",
    "    ax.set_xticklabels(ctype)\n",
    "    ax.set_title('Precision')\n",
    "    ax.set_xlim([-0.25,2.25])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.legend(loc='lower left',fontsize=10)\n",
    "    \n",
    "    ax = plt.subplot(gs[1])\n",
    "    ax.plot(cnum, recall,'o-', label=name, color=color)\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks([0,1,2, 3])\n",
    "    ax.set_xticklabels(ctype)\n",
    "    ax.set_title('Recall')\n",
    "    ax.set_xlim([-0.25,2.25])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_yticklabels([])\n",
    "    if index == 0:\n",
    "        ax.text(0.5,1.15, figname, fontsize=20, transform=ax.transAxes)\n",
    "    \n",
    "  #  ax.legend(loc='lower left',fontsize=10)\n",
    "    \n",
    "    ax = plt.subplot(gs[2])\n",
    "    ax.plot(cnum, fscore,'o-', label=name, color=color)\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks([0,1,2, 3])\n",
    "    ax.set_xticklabels(ctype)\n",
    "    ax.set_title('Fscore')\n",
    "    ax.set_xlim([-0.25,2.25])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_yticklabels([])\n",
    "   # ax.legend(loc='lower left',fontsize=10)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run ML\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import tree\n",
    "from sklearn import svm \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "if Training_set_class_frac == 1: # Do the splitting automatically instead of defined as above\n",
    "     training_features, validate_features, training_class, validate_class = train_test_split(\n",
    "     training_features, training_class, test_size=0.4, random_state=0)\n",
    "\n",
    "Scaledata = True\n",
    "\n",
    "if Scaledata:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(training_features)\n",
    "    Traindata = scaler.transform(training_features)\n",
    "    Testdata  = scaler.transform(validate_features)\n",
    "else:\n",
    "    Traindata = training_features\n",
    "    Testdata  = validate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First validate the training set\n",
    "\n",
    "classifiers = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nalpha = 10\n",
    "\n",
    "a_arr = np.logspace(-3,0,num=nalpha)\n",
    "\n",
    "for aa in range(nalpha):\n",
    "    alpha = a_arr[aa]\n",
    "    print alpha\n",
    "    classifiers['MLP alpha=%.7f'%(alpha)] = MLPClassifier(solver='lbfgs',\n",
    "                                            hidden_layer_sizes=(nfeat),\n",
    "                                            activation='logistic', alpha=alpha,\n",
    "                                                         random_state=1, tol=1e-5, \n",
    "                                                          max_iter=2000)\n",
    "        \n",
    "\n",
    "\n",
    "cnum = [0, 1, 2]\n",
    "#ctype = ['Halpha', 'OIII', 'Hbeta','OII']\n",
    "ctype = ['Halpha', 'OIII+Hbeta','OII']\n",
    "nclass = 1\n",
    "mlcolors = plt.cm.Paired(np.linspace(0,1,nalpha))\n",
    "\n",
    "#plt.figure('summary')\n",
    "\n",
    "plt.figure('performance')\n",
    "plt.rcParams['figure.figsize'] = 15, 10\n",
    "gs = gsc.GridSpec(1,3)\n",
    "gs.update(wspace=0.0, right=1.5,top=0.6)\n",
    "\n",
    "gs2 = gsc.GridSpec(1,3)\n",
    "gs2.update(wspace=0.0, right=1.5,top=0.6)\n",
    "\n",
    "\n",
    "for index in range(nalpha):\n",
    "    alpha = a_arr[index]\n",
    "    name = 'MLP alpha=%.7f'%(alpha)\n",
    "    classifier = classifiers[name]\n",
    "    print name\n",
    "    classifier.fit(Traindata, training_class)\n",
    "    y_train = classifier.predict(Traindata)\n",
    "    y_test  = classifier.predict(Testdata)\n",
    "    \n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(training_class, \n",
    "                                                                                 y_train, labels=ctype)\n",
    "    color = mlcolors[index]\n",
    "    plt.figure('performance')\n",
    "    plt.rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "    summary_performance_plot(index,precision, recall, fscore, color, cnum, ctype, name,  gs, figname='Training', ylim=[0.7,1.05])\n",
    "    \n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(validate_class, \n",
    "                                                                                 y_test, labels=ctype)\n",
    "    color = mlcolors[index]\n",
    "    plt.figure('performance2')\n",
    "    plt.rcParams['figure.figsize'] = 15, 10\n",
    "    \n",
    "    summary_performance_plot(index,precision, recall, fscore, color, cnum, ctype, name,  gs2, figname='Validation', ylim=[0.2,1.05])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#classifier = MLPClassifier(solver='lbfgs')\n",
    "C = 1.0\n",
    "kernel = 1.0 * RBF([1.0,1.0,1.0,1.0,1.0,1.0]) # for GPC\n",
    "\n",
    "classifiers = { 'MLP'                       : MLPClassifier(solver='lbfgs',\n",
    "                                                            hidden_layer_sizes=(nfeat),\n",
    "                                                           activation='logistic', alpha=0.21, random_state=1,\n",
    "                                                           tol=1e-6, max_iter=1000) ,\n",
    "                    'Random Forest'             : RandomForestClassifier(n_estimators=50),\n",
    "                    'SVC'                       : svm.SVC(),\n",
    "#                    'L1 logistic'               : LogisticRegression(C=C, penalty='l1'),\n",
    "#                    'L2 logistic (OvR)'         : LogisticRegression(C=C, penalty='l2'),\n",
    "#                    'L2 logistic (Multinomial)' : LogisticRegression(C=C, solver='lbfgs',multi_class='multinomial')\n",
    "#                    'GPC'                       : GaussianProcessClassifier(kernel)\n",
    "                    }\n",
    "\n",
    "nclass = float(len(classifiers))\n",
    "mlcolors = plt.cm.coolwarm(np.linspace(0,1,nclass))\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "cnum = [0, 1, 2]\n",
    "ctype = ['Halpha', 'OIII+Hbeta','OII']\n",
    "\n",
    "for index, (name, classifier) in enumerate(classifiers.items()):\n",
    "\n",
    "    classifier.fit(Traindata, training_class)\n",
    "    y_pred = classifier.predict(Testdata)\n",
    "\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "    #scores = cross_validate(classifier, Traindata, training_class, scoring=scoring,\n",
    "    #         cv=5, return_train_score=True)\n",
    "    #print scores\n",
    "    \n",
    "    #print 'Metrics for %s'%name\n",
    "    #print metrics.classification_report(validate_class, y_pred, labels=ctype, )\n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(validate_class, y_pred, labels=ctype)\n",
    "#    success_rate = np.zeros(3)\n",
    "\n",
    "#    for i in cnum:\n",
    "#        mm = np.where((real_val == i) & (real_val == pred_val))[0]\n",
    "#        success_rate[i] = np.float(len(mm)) / (len(np.where(real_val == i)[0]))\n",
    "\n",
    "    #plt.figure('m1%d'%index)\n",
    "    #cnf_matrix = confusion_matrix(validate_class, y_pred)\n",
    "    #plot_confusion_matrix(cnf_matrix, classes=ctype,\n",
    "    #                  title='Confusion matrix for %s, without normalization'%name, cmap = plt.cm.Blues)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#    plt.figure('m2%d'%index)\n",
    "#    plot_confusion_matrix(cnf_matrix, classes=ctype, normalize=True,\n",
    "#                      title='Normalized confusion matrix for %s'%name)\n",
    "    \n",
    "#    color = mlcolors[index]\n",
    "#    figname = 'comp'\n",
    "    plt.figure(figname)\n",
    "    summary_performance_plot(index,precision, recall, fscore, color, cnum, ctype, name,  gs2, figname='Validation', ylim=[0.2,1.05])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelining: chaining a PCA and a logistic regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(Traindata)\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_')\n",
    "\n",
    "# Prediction\n",
    "n_components = [20, 40, 64]\n",
    "Cs = np.logspace(-4, 4, 3)\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(pca__n_components=n_components,\n",
    "                              logistic__C=Cs))\n",
    "estimator.fit(Traindata, training_class)\n",
    "\n",
    "plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "plt.legend(prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(validate_class, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=ctype,\n",
    "                      title='Confusion matrix, without normalization', cmap = plt.cm.Blues)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=ctype, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
